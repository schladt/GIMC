{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277ca748",
   "metadata": {},
   "source": [
    "## Setup - Import modules and setup database connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219de2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules and setup database connection\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import requests\n",
    "\n",
    "# change to project root\n",
    "import os\n",
    "# get home directory from environment variable\n",
    "home_dir = os.environ.get('HOME')\n",
    "project_root = os.path.join(home_dir, 'projects', 'GIMC')\n",
    "os.chdir(project_root)\n",
    "\n",
    "# print current directory\n",
    "settings_file = 'settings.json'\n",
    "with open(settings_file) as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "# Database setup\n",
    "DATABASE_URL = settings['sqlalchemy_database_uri']\n",
    "engine = create_engine(DATABASE_URL, echo=False)\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "from models import Sample, Tag, Analysis, Candidate\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f4a15",
   "metadata": {},
   "source": [
    "## Count all major entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1621c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy import create_engine, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from models import Sample, Tag, Analysis, Candidate\n",
    "\n",
    "# Database setup\n",
    "DATABASE_URL = settings['sqlalchemy_database_uri']\n",
    "engine = create_engine(DATABASE_URL, echo=False)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f182c",
   "metadata": {},
   "source": [
    "## Get tags and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5030964b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tag Sample Counts:\n",
      "Tag: family=bsi, Sample Count: 3\n",
      "Tag: tatic=scheduled_task, Sample Count: 3\n",
      "Tag: ttp=wmi, Sample Count: 1\n",
      "Tag: ttp=com, Sample Count: 1\n",
      "Tag: ttp=cmd, Sample Count: 1\n",
      "Tag: family=benign, Sample Count: 5908\n",
      "Tag: class=wmi, Sample Count: 15\n",
      "Tag: class=purple, Sample Count: 1\n",
      "Tag: class=cow, Sample Count: 1\n",
      "Tag: class=com, Sample Count: 8\n",
      "Tag: class=cmd, Sample Count: 5\n"
     ]
    }
   ],
   "source": [
    "# get tags and their associated sample counts\n",
    "session.expire_all()\n",
    "from models import sample_tag\n",
    "tags = session.query(Tag).all()\n",
    "tag_sample_counts = {}\n",
    "for tag in tags:\n",
    "    sample_count = session.query(Sample).join(sample_tag).filter(sample_tag.c.tag_id == tag.id).count()\n",
    "    tag_sample_counts[(tag.key, tag.value)] = sample_count\n",
    "print(\"\\nTag Sample Counts:\")\n",
    "for tag_value, sample_count in tag_sample_counts.items():\n",
    "    print(f\"Tag: {tag_value[0]}={tag_value[1]}, Sample Count: {sample_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10a3128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples with tag (class=wmi):\n",
      "Sample ID: 3e538fdd57f368e3cbd31c14a9e1e6880c81e94c93282871c903020471a14190, Name: /mnt/data/gimc/3e/3e53/3e538fdd57f368e3cbd31c14a9e1e6880c81e94c93282871c903020471a14190\n",
      "Sample ID: f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191, Name: /mnt/data/gimc/f2/f2a8/f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191\n",
      "Sample ID: b788d12fed5dd6ad3c0331ee21e8b4c7f568b7f38116457a83370875a1315971, Name: /mnt/data/gimc/b7/b788/b788d12fed5dd6ad3c0331ee21e8b4c7f568b7f38116457a83370875a1315971\n",
      "Sample ID: 79a9409ff29b2e967161e01a0f27bcb0153a66a604e667120e30a8c09ca8deef, Name: /mnt/data/gimc/79/79a9/79a9409ff29b2e967161e01a0f27bcb0153a66a604e667120e30a8c09ca8deef\n",
      "Sample ID: c38bf4cb95005533dd52991b059bfbc60a13f81590e04735bd8a5ace221ee14b, Name: /mnt/data/gimc/c3/c38b/c38bf4cb95005533dd52991b059bfbc60a13f81590e04735bd8a5ace221ee14b\n",
      "Sample ID: 6a181382dbbf14cdab0262153bf0bcc85957f95b8d720ebe93295fe520b7cdd1, Name: /mnt/data/gimc/6a/6a18/6a181382dbbf14cdab0262153bf0bcc85957f95b8d720ebe93295fe520b7cdd1\n",
      "Sample ID: 49fa19821dc17169120ff0160580ba0053ac882f263e5d94a5fa9dd26bb1eba3, Name: /mnt/data/gimc/49/49fa/49fa19821dc17169120ff0160580ba0053ac882f263e5d94a5fa9dd26bb1eba3\n",
      "Sample ID: d078cb881b19d2e9ed54fe04985cf59f153ea14d33b20668817d44488915c019, Name: /mnt/data/gimc/d0/d078/d078cb881b19d2e9ed54fe04985cf59f153ea14d33b20668817d44488915c019\n",
      "Sample ID: 3d4049c241b839a3bb2761f634031ae53f4d7dcf7735cf035db036a3fb6cec8d, Name: /mnt/data/gimc/3d/3d40/3d4049c241b839a3bb2761f634031ae53f4d7dcf7735cf035db036a3fb6cec8d\n",
      "Sample ID: d7ba362f09d27820cee2a6a05140d02ee89b1c362c5378c41d0931a35341238e, Name: /mnt/data/gimc/d7/d7ba/d7ba362f09d27820cee2a6a05140d02ee89b1c362c5378c41d0931a35341238e\n",
      "Sample ID: ba3365452639c40e6c18255896f5d10d12d18335d7be2e1610d0b37b7575e944, Name: /mnt/data/gimc/ba/ba33/ba3365452639c40e6c18255896f5d10d12d18335d7be2e1610d0b37b7575e944\n",
      "Sample ID: 33afdf087ac21472ea67b68d1a7038ad5e170a4fc5561d4affee2a5066c7ee72, Name: /mnt/data/gimc/33/33af/33afdf087ac21472ea67b68d1a7038ad5e170a4fc5561d4affee2a5066c7ee72\n",
      "Sample ID: 2e5ce94e324f3cda8e0da23f9b71387f7c4f13793c22eb9df180e61946c425f1, Name: /mnt/data/gimc/2e/2e5c/2e5ce94e324f3cda8e0da23f9b71387f7c4f13793c22eb9df180e61946c425f1\n",
      "Sample ID: 08e482398578949fd9a23e8a7f8090a225d4801a738eb6de2dae083a3c4bff58, Name: /mnt/data/gimc/08/08e4/08e482398578949fd9a23e8a7f8090a225d4801a738eb6de2dae083a3c4bff58\n",
      "Sample ID: 1cf0572c7b6958c489da7e5c1108e46d5609740111ab7946caffdf1f9c087c6f, Name: /mnt/data/gimc/1c/1cf0/1cf0572c7b6958c489da7e5c1108e46d5609740111ab7946caffdf1f9c087c6f\n"
     ]
    }
   ],
   "source": [
    "# get all samples asscoiated with a specific tag\n",
    "session.expire_all()\n",
    "\n",
    "tag_key = 'class'\n",
    "tag_value = 'wmi'\n",
    "samples_with_tag = session.query(Sample).join(sample_tag).join(Tag).filter(Tag.key == tag_key, Tag.value == tag_value).all()\n",
    "print(f\"\\nSamples with tag ({tag_key}={tag_value}):\")\n",
    "for sample in samples_with_tag:\n",
    "    print(f\"Sample ID: {sample.sha256}, Name: {sample.filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7651b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates in the database: 28\n",
      "Number of samples in the database: 5991\n",
      "Number of analyses in the database: 17926\n",
      "Number of tags in the database: 11\n"
     ]
    }
   ],
   "source": [
    "count_Candidates = session.query(Candidate).count()\n",
    "count_samples = session.query(Sample).count()\n",
    "count_Analyses = session.query(Analysis).count()\n",
    "count_Tags = session.query(Tag).count()\n",
    "\n",
    "print(f\"Number of candidates in the database: {count_Candidates}\")\n",
    "print(f\"Number of samples in the database: {count_samples}\")\n",
    "print(f\"Number of analyses in the database: {count_Analyses}\")\n",
    "print(f\"Number of tags in the database: {count_Tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3868b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Status Counts:\n",
      "Status: 2, Count: 17378\n",
      "Status: 3, Count: 548\n"
     ]
    }
   ],
   "source": [
    "# get counts of all statuses of analyses\n",
    "session.expire_all()\n",
    "\n",
    "from sqlalchemy import func\n",
    "analysis_status_counts = session.query(Analysis.status, func.count(Analysis.id)).group_by(Analysis.status).all()\n",
    "print(\"\\nAnalysis Status Counts:\")\n",
    "for status, count in analysis_status_counts:\n",
    "    print(f\"Status: {status}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4ff7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyses for sample with SHA256 f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191:\n",
      "Analysis ID: 17848, Status: 2\n"
     ]
    }
   ],
   "source": [
    "# get analysis by sample sha256\n",
    "session.expire_all()\n",
    "\n",
    "sample_sha256 = 'f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191'\n",
    "analysis_for_sample = session.query(Analysis).join(Sample).filter(Sample.sha256 == sample_sha256).all()\n",
    "print(f\"\\nAnalyses for sample with SHA256 {sample_sha256}:\")\n",
    "for analysis in analysis_for_sample:\n",
    "    print(f\"Analysis ID: {analysis.id}, Status: {analysis.status}\")\n",
    "if not analysis_for_sample:\n",
    "    print(\"No analyses found for this sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05105727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis ID: 17849, Status: 2, Sample ID: b788d12fed5dd6ad3c0331ee21e8b4c7f568b7f38116457a83370875a1315971\n"
     ]
    }
   ],
   "source": [
    "# get analysis by its ID\n",
    "session.expire_all()\n",
    "\n",
    "analysis_id = 17849  # replace with desired analysis ID\n",
    "analysis = session.query(Analysis).filter(Analysis.id == analysis_id).first()\n",
    "if analysis:\n",
    "    print(f\"\\nAnalysis ID: {analysis.id}, Status: {analysis.status}, Sample ID: {analysis.sample}\")\n",
    "else:\n",
    "    print(f\"\\nNo analysis found with ID {analysis_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd8cfc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidates:\n",
      "Candidate ID: 33b2547820326675e8356ec6cad137b52e1b57990e66e180b4089356c7753b43, Status: 3, F1: 1.0, F2: 1.0, F3: 0.9981977343559265, Analysis ID: 17906, Error: None, Classification: cmd\n",
      "Candidate ID: baa9f40b8d15fd64663654eb3c7f299fb33b904d6a8d3fd0918fbef8a162f86b, Status: 3, F1: 1.0, F2: 1.0, F3: 0.9153369069099426, Analysis ID: 17907, Error: None, Classification: com\n",
      "Candidate ID: a4e381fdab1f71481eb33e09e9528800234c7da84a6d30f63f3339b20c03e71b, Status: 3, F1: 1.0, F2: 1.0, F3: 0.989303469657898, Analysis ID: 17908, Error: None, Classification: wmi\n",
      "Candidate ID: 7abec6d0e8b676015f472d274491b5a8d5ee0339ea4b53aa55c6a7e3694db9ee, Status: 3, F1: 0.2, F2: 0.3333333333333333, F3: 0.02028191275894642, Analysis ID: 17898, Error: None, Classification: wmi\n",
      "Candidate ID: 26493e13dfbdc807446611b1db7a3475e767c56e2024106f19446379311449ef, Status: 3, F1: 0.011363636363636364, F2: 0.0, F3: 0.0, Analysis ID: None, Error: Compilation failed: 29 errors, 0 warnings, Classification: com\n",
      "Candidate ID: a29097919f40ba277abda67cdf616d59eda96649b3b2d8d50a8b4410c642ffc6, Status: 3, F1: 1.0, F2: 0.3333333333333333, F3: 0.5240433216094971, Analysis ID: 17909, Error: None, Classification: benign\n",
      "Candidate ID: 986df2515e2de2d982d4cc076a25f4e451dcd9e16669402f39e507118eafabdc, Status: 3, F1: 0.125, F2: 0.0, F3: 0.0, Analysis ID: None, Error: Compilation failed: 2 errors, 1 warnings, Classification: com\n",
      "Candidate ID: 686cc04bdcdda1025284c24dd1eaf4e1852fc03eefcd312c53d752e8dfdd2131, Status: 3, F1: 0.125, F2: 0.0, F3: 0.0, Analysis ID: None, Error: Compilation failed: 2 errors, 1 warnings, Classification: com\n",
      "Candidate ID: 9e236cfdc58a1407f78617c664f8a60f6809c5a9ff330f8e05f30942cd7cb083, Status: 3, F1: 0.05263157894736842, F2: 0.0, F3: 0.0, Analysis ID: None, Error: Compilation failed: 6 errors, 0 warnings, Classification: com\n",
      "Candidate ID: 2dc579520150085459b63c0e2e12c823978b75094f1636ff8e7e6fb3918a03a7, Status: 3, F1: 0.1, F2: 0.0, F3: 0.0, Analysis ID: None, Error: Compilation failed: 3 errors, 0 warnings, Classification: com\n",
      "Candidate ID: a7dfe6dc7e9a1552e782042974bbbe744f44ef0ee9d885a89dc59c18464518e1, Status: 3, F1: 0.024390243902439025, F2: 0.0, F3: 0.0, Analysis ID: None, Error: Compilation failed: 6 errors, 22 warnings, Classification: com\n",
      "Candidate ID: 253592b6a8ae1870be2a53f3c2142df35ed51daaec64f68bbd7f4c2d303343cb, Status: 3, F1: 0.041666666666666664, F2: 0.0, F3: 0.0, Analysis ID: None, Error: Compilation failed: 6 errors, 5 warnings, Classification: com\n",
      "Candidate ID: 270e7901d3620991d7442424386695052f2a51612ed5f61ccfd51cdda5f9f066, Status: 3, F1: 0.011904761904761904, F2: 0.0, F3: 0.0, Analysis ID: None, Error: Compilation failed: 27 errors, 2 warnings, Classification: com\n",
      "Candidate ID: dd2934a3b7502a5344244c91e347bf71acd893379ba0b58581335309b54742cc, Status: 3, F1: 1.0, F2: 0.3333333333333333, F3: 0.10373535007238388, Analysis ID: 17915, Error: None, Classification: com\n",
      "Candidate ID: 60c11379b8ed439bbc4c0b6a27bd127d5c0425f88dc63cf518f723d8db4ba098, Status: 3, F1: 0.03333333333333333, F2: 0.0, F3: 0.0, Analysis ID: None, Error: Compilation failed: 3 errors, 20 warnings, Classification: com\n",
      "Candidate ID: c1af63166ec27eb5e350f525ad8bfacc81390c402d71d72edb8e3a9f41915c15, Status: 3, F1: 0.25, F2: 0.3333333333333333, F3: 8.109051123028621e-05, Analysis ID: 17914, Error: None, Classification: com\n",
      "Candidate ID: 1960ded4679480f2a27da30ddd90f4f783a5dd35a893abe1b56be31ab23c38a2, Status: 3, F1: 1.0, F2: 0.3333333333333333, F3: 0.10373535007238388, Analysis ID: 17918, Error: None, Classification: com\n",
      "Candidate ID: 87be6d6079efda229cf0062de236c848ca496b45a1845d0b28c2a9f64763d968, Status: 3, F1: 1.0, F2: 0.3333333333333333, F3: 0.20102089643478394, Analysis ID: 17917, Error: None, Classification: com\n",
      "Candidate ID: b81a26eeb14855014ee7d6b7a30dbd992d86b584ccff2021378656b18426a589, Status: 3, F1: 0.0625, F2: 0.0, F3: 0.0, Analysis ID: None, Error: Compilation failed: 5 errors, 0 warnings, Classification: com\n",
      "Candidate ID: 6d250a1cd1d2cfae7b102a6f402ec65daad88b50afb93e904c4556e6b3b0d108, Status: 3, F1: 1.0, F2: 0.3333333333333333, F3: 0.28169354796409607, Analysis ID: 17921, Error: None, Classification: com\n",
      "Candidate ID: 49a9331ade8e4c805aadfdeb217306c24de80260318577073b298e48438d0809, Status: 3, F1: 1.0, F2: 0.3333333333333333, F3: 0.10373535007238388, Analysis ID: 17920, Error: None, Classification: com\n",
      "Candidate ID: 29ec6767996030470045af828e77b157d2dc83655f2b080994701e475dade00e, Status: 3, F1: 1.0, F2: 0.3333333333333333, F3: 0.10269278287887573, Analysis ID: 17916, Error: None, Classification: com\n",
      "Candidate ID: 358562df20ece1c83404e3fff49294b099225855e3daaf5cbbc2a21bbad7f989, Status: 3, F1: 1.0, F2: 0.3333333333333333, F3: 0.10373535007238388, Analysis ID: 17923, Error: None, Classification: com\n",
      "Candidate ID: 18d40e4a099d4c8e6942dbc4253ef367ad8786c0e10cefa95873ae252a15a0a0, Status: 3, F1: 1.0, F2: 0.0, F3: 0.2699926197528839, Analysis ID: 17919, Error: None, Classification: com\n",
      "Candidate ID: dd2296f33a5069e2af4aca8b146aa3d15941fc9dd042a459014aa446d05b465f, Status: 3, F1: 1.0, F2: 0.3333333333333333, F3: 0.041168566793203354, Analysis ID: 17922, Error: None, Classification: com\n",
      "Candidate ID: e1fbc14bb9f6c4bb7e80579b6298e49461f8ae5561779d546b783c829344ec04, Status: 3, F1: 0.1, F2: 0.0, F3: 0.0, Analysis ID: None, Error: Compilation failed: 2 errors, 3 warnings, Classification: com\n",
      "Candidate ID: 86a5636bf56188a3d43ca91baefcf92c7b3df050a89aa5de3a0e7b5e1eb21baa, Status: 3, F1: 1.0, F2: 0.0, F3: 0.11633467674255371, Analysis ID: 17926, Error: None, Classification: com\n",
      "Candidate ID: 6d7b3e36112f6a21a8d4fc842d79cb9a88c2321012ebc497361d807987c03d3d, Status: 3, F1: 1.0, F2: 0.3333333333333333, F3: 0.10269278287887573, Analysis ID: 17927, Error: None, Classification: com\n"
     ]
    }
   ],
   "source": [
    "# get candidates\n",
    "session.expire_all()\n",
    "\n",
    "candidates = session.query(Candidate).all()\n",
    "print(\"\\nCandidates:\")\n",
    "for candidate in candidates:\n",
    "    print(f\"Candidate ID: {candidate.hash}, Status: {candidate.status}, F1: {candidate.F1}, F2: {candidate.F2}, F3: {candidate.F3}, Analysis ID: {candidate.analysis_id}, Error: {candidate.error_message}, Classification: {candidate.classification}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4921f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # update status of a candidate to 0\n",
    "# session.expire_all()\n",
    "\n",
    "# candidate_hash = 'a4e381fdab1f71481eb33e09e9528800234c7da84a6d30f63f3339b20c03e71b'  # replace with actual candidate hash\n",
    "# candidate = session.query(Candidate).filter(Candidate.hash == candidate_hash).first()\n",
    "# if candidate:\n",
    "#     candidate.status = 0  # replace with desired status\n",
    "#     candidate.F1 = None\n",
    "#     candidate.F2 = None\n",
    "#     candidate.F3 = None\n",
    "#     candidate.analysis_id = None\n",
    "#     candidate.error_message = None\n",
    "#     session.commit()\n",
    "#     print(f\"\\nUpdated candidate {candidate_hash} to status '{candidate.status}'.\")\n",
    "# else:\n",
    "#     print(f\"\\nNo candidate found with hash {candidate_hash}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef331e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Details for a4e381fdab1f71481eb33e09e9528800234c7da84a6d30f63f3339b20c03e71b:\n",
      "Hash: a4e381fdab1f71481eb33e09e9528800234c7da84a6d30f63f3339b20c03e71b\n",
      "Status: 3\n",
      "F1: 1.0\n",
      "F2: 1.0\n",
      "F3: 0.989303469657898\n",
      "Classification: wmi\n",
      "Analysis ID: 17908\n"
     ]
    }
   ],
   "source": [
    "# get full candidate details\n",
    "session.expire_all()\n",
    "\n",
    "import base64\n",
    "candidate_hash = 'a4e381fdab1f71481eb33e09e9528800234c7da84a6d30f63f3339b20c03e71b'  # replace with actual candidate hash\n",
    "candidate = session.query(Candidate).filter(Candidate.hash == candidate_hash).first()\n",
    "if candidate:\n",
    "    print(f\"\\nCandidate Details for {candidate_hash}:\")\n",
    "    print(f\"Hash: {candidate.hash}\")\n",
    "    print(f\"Status: {candidate.status}\")\n",
    "    print(f\"F1: {candidate.F1}\")\n",
    "    print(f\"F2: {candidate.F2}\")\n",
    "    print(f\"F3: {candidate.F3}\")\n",
    "    print(f\"Classification: {candidate.classification}\")\n",
    "    print(f\"Analysis ID: {candidate.analysis_id}\")\n",
    "    # unbase64 the code snippet and print first XX characters\n",
    "    decoded_code = base64.b64decode(candidate.code).decode('utf-8', errors='ignore')\n",
    "    # print(f\"Code Snippet: {decoded_code[:500]}\")\n",
    "else:\n",
    "    print(f\"\\nNo candidate found with hash {candidate_hash}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cd1380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification inference example\n",
    "\n",
    "# get sample from analysis ID\n",
    "# session.expire_all()\n",
    "\n",
    "# analysis_id = 5000  # replace with desired analysis ID\n",
    "# analysis = session.query(Analysis).filter(Analysis.id == analysis_id).first()\n",
    "# if analysis:\n",
    "#     sample = session.query(Sample).filter(Sample.sha256 == analysis.sample).first()\n",
    "#     if sample:\n",
    "#         print(f\"\\nSample for Analysis ID {analysis_id}:\")\n",
    "#         print(f\"Sample ID: {sample.sha256}, Name: {sample.filepath}\")\n",
    "#     else:\n",
    "#         print(f\"\\nNo sample found for Analysis ID {analysis_id}.\")\n",
    "#     # get all tags for the sample\n",
    "#     tags = session.query(Tag).join(sample_tag).join(Sample).filter(Sample.sha256 == sample.sha256).all()\n",
    "#     print(f\"\\nTags for Sample ID {sample.sha256}:\")\n",
    "#     for tag in tags:\n",
    "#         print(f\"Tag: {tag.key}={tag.value}\")\n",
    "\n",
    "# import torch\n",
    "# from classifier.models.cnn_nlp import CNN_NLP\n",
    "\n",
    "# DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# from transformers import AutoTokenizer\n",
    "# classifier_path = '/mnt/data/gimc/classifier/model_data/cnn4bsi_checkpoint.pth'\n",
    "# tokenizer_path = '/mnt/data/gimc/classifier/model_data/mal-reformer'\n",
    "\n",
    "# TOKENIZER = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "# TOKENIZER.pad_token = \"[PAD]\"\n",
    "# TOKENIZER.cls_token = \"[CLS]\"\n",
    "# TOKENIZER.sep_token = \"[SEP]\"\n",
    "\n",
    "# checkpoint = torch.load(classifier_path)\n",
    "# vocab_size = 20000\n",
    "# embed_dim = 128\n",
    "# num_classes = 4\n",
    "# dropout = 0.5\n",
    "# MODEL = CNN_NLP(\n",
    "#     pretrained_embedding=None,\n",
    "#     freeze_embedding=False,\n",
    "#     vocab_size=vocab_size,\n",
    "#     embed_dim=embed_dim,\n",
    "#     filter_sizes=[3, 4, 5],\n",
    "#     num_filters=[10, 10, 10],\n",
    "#     num_classes=num_classes,\n",
    "#     dropout=dropout\n",
    "# )\n",
    "# MODEL.load_state_dict(checkpoint['model_states'][-1])\n",
    "# MODEL.to(DEVICE)\n",
    "# MODEL.eval()\n",
    "\n",
    "# def mal_tokenizer(line):\n",
    "#     \"\"\"\n",
    "#     Tokenize a line of text\n",
    "#     \"\"\"\n",
    "#     line = line.lower()\n",
    "#     line = line.replace(',', ' ')\n",
    "#     line = line.replace('\\\\', ' ')\n",
    "#     line = line.replace('\\\\\\\\', ' ')\n",
    "#     return line.split()\n",
    "\n",
    "# dynamic_report_tokenized = []\n",
    "\n",
    "# with open(analysis.report) as f:\n",
    "#     report = f.read()\n",
    "#     dynamic_report = json.loads(report)['dynamic']\n",
    "#     for item in dynamic_report:\n",
    "#         line = f\"{item['Operation']}, {item['Path']}, {item['Result']}\"\n",
    "#         dynamic_report_tokenized.extend(mal_tokenizer(line))\n",
    "\n",
    "# report_text = \" \".join(dynamic_report_tokenized)\n",
    "\n",
    "# MAX_SEQUENCE_LENGTH = 20480 * 2\n",
    "# inputs = TOKENIZER(\n",
    "#     report_text,\n",
    "#     padding='max_length',\n",
    "#     truncation=True,\n",
    "#     max_length=MAX_SEQUENCE_LENGTH,\n",
    "#     return_tensors='pt'\n",
    "# ).to(DEVICE)\n",
    "\n",
    "# input_ids = inputs['input_ids']\n",
    "\n",
    "# # Run inference\n",
    "# with torch.no_grad():\n",
    "#     logits = MODEL(input_ids)\n",
    "#     # Apply softmax to get probabilities\n",
    "#     probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "# # turn props into a list of probabilities\n",
    "# probabilities = probs.cpu().numpy().flatten().tolist()\n",
    "# print(\"Class Probabilities:\")\n",
    "# for i, prob in enumerate(probabilities):\n",
    "#     print(f\"Class {i}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33fd7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OllamaChat class for interacting with local Ollama server\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1114d669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are assisting a security researcher in generating safe research code to demonstrate various Wind\n",
      "\n",
      "\n",
      "Generate 3 different C/C++ implementations that statisfy the following objectives:\n",
      "Establish persi\n"
     ]
    }
   ],
   "source": [
    "from genetic_improvement.config import SYSTEM_PROMPT, USER_PROMPT\n",
    "print(SYSTEM_PROMPT[:100])\n",
    "print(USER_PROMPT[:100])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genetic_improvement.ollamachat import OllamaChat\n",
    "\n",
    "ollama_chat = OllamaChat(model=\"qwen3-coder:latest\", system_prompt=SYSTEM_PROMPT, temperature=0.3)\n",
    "chat_response = ollama_chat.chat(user_text=USER_PROMPT, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = OllamaChat.parse_variants(chat_response)\n",
    "print(len(variants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9715ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send variants to evaluation server\n",
    "unit_test_code_path = \"/home/mike/projects/GIMC/behavioral_subsets/scheduled_execution/test_scheduled_execution.py\"\n",
    "with open(unit_test_code_path, \"r\") as f:\n",
    "    unit_test_code = f.read()\n",
    "\n",
    "seed_candidates = []\n",
    "classification = 'com' # example classification target\n",
    "TOKEN = settings['sandbox_token']\n",
    "ES_SERVER = settings['evaluation_server']\n",
    "\n",
    "for variant in variants:\n",
    "    code_content = variant['code']\n",
    "    makefile_content = variant['makefile']\n",
    "\n",
    "    # Base64 encode the code and makefile content\n",
    "    encoded_code = base64.b64encode(code_content.encode('utf-8')).decode('utf-8')\n",
    "    encoded_makefile = base64.b64encode(makefile_content.encode('utf-8')).decode('utf-8')\n",
    "    encoded_unittest = base64.b64encode(unit_test_code.encode('utf-8')).decode('utf-8')\n",
    "\n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        'code': encoded_code,\n",
    "        'class': classification,\n",
    "        'makefile': encoded_makefile,\n",
    "        'unittest': encoded_unittest\n",
    "    }\n",
    "\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "\n",
    "    # Send the POST request to the evaluation server\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            ES_SERVER + '/submit',\n",
    "            json=payload,\n",
    "            headers=headers\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        print(\"Code submitted successfully. Server response:\")\n",
    "        # get candidate_hash from response and retrieve candidate details\n",
    "        response_data = response.json()\n",
    "        candidate_hash = response_data.get('candidate_hash')\n",
    "        if candidate_hash:\n",
    "            candidate = session.query(Candidate).filter(Candidate.hash == candidate_hash).first()\n",
    "            if candidate:\n",
    "                seed_candidates.append(candidate)\n",
    "                print(f'Candidate Added: {candidate.hash}, Status: {candidate.status}, Classification: {candidate.classification}')\n",
    "                \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error submitting code: {e}\")\n",
    "        print(\"Failed to submit code. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938109cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for candidate in seed_candidates:\n",
    "    # refresh candidate from database\n",
    "    session.expire(candidate)\n",
    "    print(f'Seed Candidate: {candidate.hash}, Status: {candidate.status}, Classification: {candidate.classification}, F1: {candidate.F1}, F2: {candidate.F2}, F3: {candidate.F3}, error: {candidate.error_message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e2759",
   "metadata": {},
   "outputs": [],
   "source": [
    "makefile = seed_candidates[0].makefile\n",
    "# decode from base64\n",
    "decoded_makefile = base64.b64decode(makefile).decode('utf-8', errors='ignore')\n",
    "print(decoded_makefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2209c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = seed_candidates[0].code\n",
    "# decode from base64\n",
    "decoded_code = base64.b64decode(code).decode('utf-8', errors='ignore')\n",
    "print(decoded_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a91dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to parent directory\n",
    "import os\n",
    "\n",
    "# get home directory\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "os.chdir(os.path.join(home_dir, \"projects\", \"GIMC\"))\n",
    "\n",
    "from genetic_improvement.genome import Genome\n",
    "\n",
    "xml = Genome.to_xml(decoded_code, \"C++\")\n",
    "print(xml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c2207",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Genome()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
