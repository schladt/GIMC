{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, select, MetaData, Table\n",
    "from sqlalchemy.ext.automap import automap_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../settings.json') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "db_uri = settings['sqlalchemy_database_uri']\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 20000\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if json files exist\n",
    "if not os.path.isfile('reports.json'):\n",
    "    # connect to database\n",
    "    engine = create_engine(db_uri)\n",
    "\n",
    "    # load tables\n",
    "    metadata_obj = MetaData()\n",
    "    conn = engine.connect()\n",
    "\n",
    "    Tag = Table('tag', metadata_obj, autoload_with=engine)\n",
    "    SampleTag = Table('sample_tag', metadata_obj, autoload_with=engine)\n",
    "    Analysis = Table('analysis', metadata_obj, autoload_with=engine)\n",
    "\n",
    "    # Start a session\n",
    "    session = Session(engine)\n",
    "\n",
    "    # get all reports with the tag 'file_search_dfs'\n",
    "    stmt = select(Analysis.c.report).join(SampleTag, Tag.c.id == SampleTag.c.tag_id).join(\n",
    "        Analysis, SampleTag.c.sample_id == Analysis.c.sample).where(\n",
    "        Tag.c.value == 'file_search_dfs'\n",
    "    )\n",
    "\n",
    "    results = session.execute(stmt).fetchall()\n",
    "    dfs_report_paths = [r[0] for r in results] \n",
    "\n",
    "    # get all reports with the tag 'file_search_bfs'\n",
    "    stmt = select(Analysis.c.report).join(SampleTag, Tag.c.id == SampleTag.c.tag_id).join(\n",
    "        Analysis, SampleTag.c.sample_id == Analysis.c.sample).where(\n",
    "        Tag.c.value == 'file_search_bfs'\n",
    "    )\n",
    "\n",
    "    results = session.execute(stmt).fetchall()\n",
    "    bfs_report_paths = [r[0] for r in results]\n",
    "\n",
    "    # Close the session\n",
    "    session.close()\n",
    "\n",
    "    # fetch reports\n",
    "    bfs_reports = []\n",
    "    for report_path in tqdm(bfs_report_paths, desc=\"Reading BFS reports\"):\n",
    "        with open(report_path) as f:\n",
    "            bfs_reports.append(f.read())\n",
    "\n",
    "    dfs_reports = []\n",
    "    for report_path in tqdm(dfs_report_paths, desc=\"Reading DFS reports\"):\n",
    "        with open(report_path) as f:\n",
    "            dfs_reports.append(f.read())\n",
    "\n",
    "    # combine reports\n",
    "    reports = [[r, 'bfs'] for r in bfs_reports] + [[r, 'dfs'] for r in dfs_reports]\n",
    "\n",
    "    # shuffle reports\n",
    "    random.shuffle(reports)\n",
    "\n",
    "    # Preprocess reports\n",
    "    i = 0    \n",
    "    for report in tqdm(reports, desc=\"Preprocessing reports\"):\n",
    "        report = report[0]\n",
    "        # replace all non-alphanumeric characters with spaces\n",
    "        report = re.sub(r'[^\\w\\s.-]+', ' ', report)\n",
    "        # split report into words\n",
    "        report = report.split()\n",
    "        # remove strings less than 3 characters long\n",
    "        report = [word.lower() for word in report if len(word) > 2]\n",
    "        reports[i][0] = ' '.join(report)\n",
    "        i += 1\n",
    "\n",
    "    # json dump reports to file\n",
    "    with open('reports.json', 'w') as f:\n",
    "        json.dump(reports, f)\n",
    "\n",
    "else:\n",
    "    # load reports from file\n",
    "    with open('reports.json') as f:\n",
    "        reports = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training, validation, and test sets\n",
    "\n",
    "from torchdata.datapipes.iter import IterableWrapper\n",
    "dp = IterableWrapper(reports)\n",
    "\n",
    "# Get the number of rows in dataset\n",
    "N_ROWS = len(list(dp)) \n",
    "N_train = int(N_ROWS * 0.8)\n",
    "N_valid = int(N_ROWS * 0.1)\n",
    "N_test = N_ROWS - N_train - N_valid\n",
    "\n",
    "# Split into training and val datapipes early on. Will build vocabulary from training datapipe only.\n",
    "train_dp, valid_dp, test_dp = dp.random_split(total_length=N_ROWS, weights={\"train\": N_train, \"valid\": N_valid, \"test\": N_test}, seed=RANDOM_SEED)\n",
    "\n",
    "print(f'Num Train: {len(train_dp)}')\n",
    "print(f'Num Validate: {len(valid_dp)}')\n",
    "print(f'Num Test: {len(test_dp)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(tokenizer='basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dp), specials=[\"<unk>\", \"<pad>\"], max_tokens=VOCABULARY_SIZE)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "PADDING_VALUE=vocab['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_transform = lambda x: 1 if x == 'dfs' else 0\n",
    "\n",
    "# Print out the output of text_transform\n",
    "print(\"input to the text_transform:\", \"here is an example\")\n",
    "print(\"output of the text_transform:\", text_transform(list(train_dp)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_dp)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "# following two helper functions from https://medium.com/@bitdribble/migrate-torchtext-to-the-new-0-9-0-api-1ff1472b5d71\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, label_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        processed_text = torch.tensor(text_transform(_text))\n",
    "        text_list.append(processed_text)\n",
    "        label_list.append(label_transform(_label))\n",
    "    return pad_sequence(text_list, padding_value=PADDING_VALUE).to(DEVICE), torch.tensor(label_list).to(DEVICE)\n",
    "\n",
    "class BatchSamplerSimilarLength(Sampler):\n",
    "    def __init__(self, dataset, batch_size, indices=None, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        # get the indices and length\n",
    "        self.indices = [(i, len(tokenizer(s[1]))) for i, s in enumerate(dataset)]\n",
    "        # if indices are passed, then use only the ones passed (for ddp)\n",
    "        if indices is not None:\n",
    "            self.indices = torch.tensor(self.indices)[indices].tolist()\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.indices)\n",
    "\n",
    "        pooled_indices = []\n",
    "        # create pool of indices with similar lengths\n",
    "        for i in range(0, len(self.indices), self.batch_size * 100):\n",
    "            pooled_indices.extend(sorted(self.indices[i:i + self.batch_size * 100], key=lambda x: x[1]))\n",
    "        self.pooled_indices = [x[0] for x in pooled_indices]\n",
    "\n",
    "        # yield indices for current batch\n",
    "        batches = [self.pooled_indices[i:i + self.batch_size] for i in\n",
    "               range(0, len(self.pooled_indices), self.batch_size)]\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.shuffle(batches)\n",
    "        for batch in batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pooled_indices) // self.batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dp_list = list(train_dp)\n",
    "valid_dp_list = list(valid_dp)\n",
    "test_dp_list = list(test_dp)\n",
    "\n",
    "train_loader = DataLoader(train_dp_list, \n",
    "                          batch_sampler=BatchSamplerSimilarLength(dataset = train_dp_list, \n",
    "                                                                  batch_size=BATCH_SIZE),\n",
    "                          collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(train_dp_list, \n",
    "                          batch_sampler=BatchSamplerSimilarLength(dataset = valid_dp_list, \n",
    "                                                                  batch_size=BATCH_SIZE,\n",
    "                                                                  shuffle=False),\n",
    "                          collate_fn=collate_batch)\n",
    "test_loader = DataLoader(train_dp_list, \n",
    "                          batch_sampler=BatchSamplerSimilarLength(dataset = test_dp_list, \n",
    "                                                                  batch_size=BATCH_SIZE,\n",
    "                                                                  shuffle=False),\n",
    "                          collate_fn=collate_batch)\n",
    "\n",
    "text_batch, label_batch = next(iter(train_loader))\n",
    "print(text_batch.size())\n",
    "print(label_batch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train')\n",
    "for text_batch, label_batch in train_loader:\n",
    "    print(f'Text matrix size: {text_batch.size()}')\n",
    "    print(f'Target vector size: {label_batch.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nValid:')\n",
    "for text_batch, label_batch in valid_loader:\n",
    "    print(f'Text matrix size: {text_batch.size()}')\n",
    "    print(f'Target vector size: {label_batch.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nTest:')\n",
    "for text_batch, label_batch in test_loader:\n",
    "    print(f'Text matrix size: {text_batch.size()}')\n",
    "    print(f'Target vector size: {label_batch.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum().to(\"cpu\")\n",
    "\n",
    "    return float(correct_pred)/num_examples * 100\n",
    "\n",
    "\n",
    "def train_model(model, num_epochs, train_loader,\n",
    "                valid_loader, test_loader, optimizer,\n",
    "                device, logging_interval=50,\n",
    "                scheduler=None,\n",
    "                scheduler_on='valid_acc'):\n",
    "\n",
    "    start_time = time.time()\n",
    "    minibatch_loss_list, train_acc_list, valid_acc_list = [], [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # ## FORWARD AND BACK PROP\n",
    "            logits = model(features)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # ## UPDATE MODEL PARAMETERS\n",
    "            optimizer.step()\n",
    "\n",
    "            # ## LOGGING\n",
    "            minibatch_loss_list.append(loss.item())\n",
    "            if not batch_idx % logging_interval:\n",
    "                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
    "                      f'| Batch {batch_idx:04d}/{len(train_loader):04d} '\n",
    "                      f'| Loss: {loss:.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # save memory during inference\n",
    "            train_acc = compute_accuracy(model, train_loader, device=device)\n",
    "            valid_acc = compute_accuracy(model, valid_loader, device=device)\n",
    "            print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
    "                  f'| Train: {train_acc :.2f}% '\n",
    "                  f'| Validation: {valid_acc :.2f}%')\n",
    "            train_acc_list.append(train_acc)\n",
    "            valid_acc_list.append(valid_acc)\n",
    "\n",
    "        elapsed = (time.time() - start_time)/60\n",
    "        print(f'Time elapsed: {elapsed:.2f} min')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "\n",
    "            if scheduler_on == 'valid_acc':\n",
    "                scheduler.step(valid_acc_list[-1])\n",
    "            elif scheduler_on == 'minibatch_loss':\n",
    "                scheduler.step(minibatch_loss_list[-1])\n",
    "            else:\n",
    "                raise ValueError(f'Invalid `scheduler_on` choice.')\n",
    "        \n",
    "\n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Total Training Time: {elapsed:.2f} min')\n",
    "\n",
    "    test_acc = compute_accuracy(model, test_loader, device=device)\n",
    "    print(f'Test accuracy {test_acc :.2f}%')\n",
    "\n",
    "    return minibatch_loss_list, train_acc_list, valid_acc_list\n",
    "\n",
    "def plot_training_loss(minibatch_loss_list, num_epochs, iter_per_epoch,\n",
    "                       results_dir=None, averaging_iterations=100):\n",
    "\n",
    "    plt.figure()\n",
    "    ax1 = plt.subplot(1, 1, 1)\n",
    "    ax1.plot(range(len(minibatch_loss_list)),\n",
    "             (minibatch_loss_list), label='Minibatch Loss')\n",
    "\n",
    "    if len(minibatch_loss_list) > 1000:\n",
    "        ax1.set_ylim([\n",
    "            0, np.max(minibatch_loss_list[1000:])*1.5\n",
    "            ])\n",
    "    ax1.set_xlabel('Iterations')\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    ax1.plot(np.convolve(minibatch_loss_list,\n",
    "                         np.ones(averaging_iterations,)/averaging_iterations,\n",
    "                         mode='valid'),\n",
    "             label='Running Average')\n",
    "    ax1.legend()\n",
    "\n",
    "    ###################\n",
    "    # Set scond x-axis\n",
    "    ax2 = ax1.twiny()\n",
    "    newlabel = list(range(num_epochs+1))\n",
    "\n",
    "    newpos = [e*iter_per_epoch for e in newlabel]\n",
    "\n",
    "    ax2.set_xticks(newpos[::10])\n",
    "    ax2.set_xticklabels(newlabel[::10])\n",
    "\n",
    "    ax2.xaxis.set_ticks_position('bottom')\n",
    "    ax2.xaxis.set_label_position('bottom')\n",
    "    ax2.spines['bottom'].set_position(('outward', 45))\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    ###################\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if results_dir is not None:\n",
    "        image_path = os.path.join(results_dir, 'plot_training_loss.pdf')\n",
    "        plt.savefig(image_path)\n",
    "\n",
    "\n",
    "def plot_accuracy(train_acc_list, valid_acc_list, results_dir):\n",
    "\n",
    "    num_epochs = len(train_acc_list)\n",
    "\n",
    "    plt.plot(np.arange(1, num_epochs+1),\n",
    "             train_acc_list, label='Training')\n",
    "    plt.plot(np.arange(1, num_epochs+1),\n",
    "             valid_acc_list, label='Validation')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if results_dir is not None:\n",
    "        image_path = os.path.join(\n",
    "            results_dir, 'plot_acc_training_validation.pdf')\n",
    "        plt.savefig(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
    "                                 hidden_dim)        \n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, text):\n",
    "        # text dim: [sentence length, batch size]        \n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "\n",
    "        hidden.squeeze_(0)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "        \n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim=len(vocab),\n",
    "             embedding_dim=EMBEDDING_DIM,\n",
    "             hidden_dim=HIDDEN_DIM,\n",
    "             output_dim=NUM_CLASSES # could use 1 for binary classification\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       mode='max',\n",
    "                                                       verbose=True)\n",
    "\n",
    "minibatch_loss_list, train_acc_list, valid_acc_list = train_model(\n",
    "    model=model,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    logging_interval=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
