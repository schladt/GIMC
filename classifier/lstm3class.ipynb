{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, select, MetaData, Table\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "\n",
    "from utils.batch import collate_batch, BatchSamplerSimilarLength\n",
    "from utils.train import train_model, compute_accuracy\n",
    "from utils.plot import plot_accuracy, plot_training_loss\n",
    "\n",
    "from models.lstm import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../settings.json') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "db_uri = settings['sqlalchemy_database_uri']\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 20000\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(line):\n",
    "    line = line.lower()\n",
    "    line = line.replace(',', ' ')\n",
    "    line = line.replace('\\\\', ' ')\n",
    "    line = line.replace('\\\\\\\\', ' ')\n",
    "    return line.split()\n",
    "\n",
    "tokenizer = get_tokenizer(tokenizer=custom_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if json files exist\n",
    "if not os.path.isfile('reports.json'):\n",
    "    # connect to database\n",
    "    engine = create_engine(db_uri)\n",
    "\n",
    "    # load tables\n",
    "    metadata_obj = MetaData()\n",
    "    conn = engine.connect()\n",
    "\n",
    "    Tag = Table('tag', metadata_obj, autoload_with=engine)\n",
    "    SampleTag = Table('sample_tag', metadata_obj, autoload_with=engine)\n",
    "    Analysis = Table('analysis', metadata_obj, autoload_with=engine)\n",
    "\n",
    "    # Start a session\n",
    "    session = Session(engine)\n",
    "\n",
    "    # get all reports with the tag 'file_search_dfs'\n",
    "    stmt = select(Analysis.c.report).join(SampleTag, Tag.c.id == SampleTag.c.tag_id).join(\n",
    "        Analysis, SampleTag.c.sample_id == Analysis.c.sample).where(\n",
    "        Tag.c.value == 'file_search_dfs'\n",
    "    )\n",
    "\n",
    "    results = session.execute(stmt).fetchall()\n",
    "    dfs_report_paths = [r[0] for r in results] \n",
    "\n",
    "    # get all reports with the tag 'file_search_bfs'\n",
    "    stmt = select(Analysis.c.report).join(SampleTag, Tag.c.id == SampleTag.c.tag_id).join(\n",
    "        Analysis, SampleTag.c.sample_id == Analysis.c.sample).where(\n",
    "        Tag.c.value == 'file_search_bfs'\n",
    "    )\n",
    "\n",
    "    results = session.execute(stmt).fetchall()\n",
    "    bfs_report_paths = [r[0] for r in results]\n",
    "\n",
    "    # get all reports with the tag 'benign'\n",
    "    stmt = select(Analysis.c.report).join(SampleTag, Tag.c.id == SampleTag.c.tag_id).join(\n",
    "        Analysis, SampleTag.c.sample_id == Analysis.c.sample).where(\n",
    "        Tag.c.value == 'benign'\n",
    "    )\n",
    "\n",
    "    results = session.execute(stmt).fetchall()\n",
    "    benign_report_paths = [r[0] for r in results]\n",
    "\n",
    "    # Close the session\n",
    "    session.close()\n",
    "\n",
    "    # fetch reports\n",
    "    bfs_reports = []\n",
    "    for report_path in tqdm(bfs_report_paths, desc=\"Reading BFS reports\"):\n",
    "        with open(report_path) as f:\n",
    "            bfs_reports.append(f.read())\n",
    "\n",
    "    dfs_reports = []\n",
    "    for report_path in tqdm(dfs_report_paths, desc=\"Reading DFS reports\"):\n",
    "        with open(report_path) as f:\n",
    "            dfs_reports.append(f.read())\n",
    "\n",
    "    benign_reports = []\n",
    "    for report_path in tqdm(benign_report_paths, desc=\"Reading benign reports\"):\n",
    "        with open(report_path) as f:\n",
    "            benign_reports.append(f.read())\n",
    "\n",
    "    # combine reports\n",
    "    reports = [[r, 'bfs'] for r in bfs_reports] + [[r, 'dfs'] for r in dfs_reports] + [[r, 'benign'] for r in benign_reports]\n",
    "\n",
    "    # shuffle reports\n",
    "    random.shuffle(reports)\n",
    "\n",
    "    # Tokenize reports\n",
    "    i = 0\n",
    "    for report in tqdm(reports, desc=\"Tokenizing reports\"):\n",
    "        dynamic_report = json.loads(report[0])['dynamic']\n",
    "        dynamic_report_tokenized = []\n",
    "        for item in dynamic_report:\n",
    "            line = f\"{item['Operation']}, {item['Path']}, {item['Result']}\"\n",
    "            dynamic_report_tokenized.extend(tokenizer(line))\n",
    "        reports[i][0] = dynamic_report_tokenized\n",
    "        i += 1\n",
    "\n",
    "    # json dump reports to file\n",
    "    print(\"Dumping reports to file\")\n",
    "    with open('reports.json', 'w') as f:\n",
    "        json.dump(reports, f)\n",
    "\n",
    "else:\n",
    "    print(\"Loading reports from file\")\n",
    "    # load reports from file\n",
    "    with open('reports.json') as f:\n",
    "        reports = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training, validation, and test sets\n",
    "from torchdata.datapipes.iter import IterableWrapper\n",
    "dp = IterableWrapper(reports)\n",
    "\n",
    "# Get the number of rows in dataset\n",
    "N_ROWS = len(list(dp)) \n",
    "N_train = int(N_ROWS * 0.8)\n",
    "N_valid = int(N_ROWS * 0.1)\n",
    "N_test = N_ROWS - N_train - N_valid\n",
    "\n",
    "# Split into training and val datapipes early on. Will build vocabulary from training datapipe only.\n",
    "train_dp, valid_dp, test_dp = dp.random_split(total_length=N_ROWS, weights={\"train\": N_train, \"valid\": N_valid, \"test\": N_test}, seed=RANDOM_SEED)\n",
    "\n",
    "print(f'Num Train: {len(train_dp)}')\n",
    "print(f'Num Validate: {len(valid_dp)}')\n",
    "print(f'Num Test: {len(test_dp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield text\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dp), specials=[\"<unk>\", \"<pad>\"], max_tokens=VOCABULARY_SIZE)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "PADDING_VALUE=vocab['<PAD>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define text and label transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform = lambda x: [vocab[token] for token in x]\n",
    "label_transform = lambda x: 0 if x == 'dfs' else 1 if x == 'bfs' else 2\n",
    "\n",
    "# Print out the output of text_transform\n",
    "print(\"input to the text_transform:\", \"here is an example\")\n",
    "print(\"output of the text_transform:\", text_transform(list(train_dp)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_batch_wrapper(batch):\n",
    "    return collate_batch(batch=batch, \n",
    "                  padding_value=PADDING_VALUE, \n",
    "                  device=DEVICE, \n",
    "                  text_transform=text_transform, \n",
    "                  label_transform=label_transform)\n",
    "\n",
    "train_dp_list = list(train_dp)\n",
    "valid_dp_list = list(valid_dp)\n",
    "test_dp_list = list(test_dp)\n",
    "\n",
    "train_loader = DataLoader(train_dp_list, \n",
    "                          batch_sampler=BatchSamplerSimilarLength(dataset = train_dp_list, batch_size=BATCH_SIZE),\n",
    "                          collate_fn=collate_batch_wrapper)\n",
    "valid_loader = DataLoader(train_dp_list, \n",
    "                          batch_sampler=BatchSamplerSimilarLength(dataset = valid_dp_list, batch_size=BATCH_SIZE, shuffle=False),\n",
    "                          collate_fn=collate_batch_wrapper)\n",
    "test_loader = DataLoader(train_dp_list, \n",
    "                          batch_sampler=BatchSamplerSimilarLength(dataset = test_dp_list, batch_size=BATCH_SIZE, shuffle=False),\n",
    "                          collate_fn=collate_batch_wrapper)\n",
    "\n",
    "text_batch, label_batch = next(iter(train_loader))\n",
    "print(text_batch.size())\n",
    "print(label_batch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train')\n",
    "for text_batch, label_batch in train_loader:\n",
    "    print(f'Text matrix size: {text_batch.size()}')\n",
    "    print(f'Target vector size: {label_batch.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nValid:')\n",
    "for text_batch, label_batch in valid_loader:\n",
    "    print(f'Text matrix size: {text_batch.size()}')\n",
    "    print(f'Target vector size: {label_batch.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nTest:')\n",
    "for text_batch, label_batch in test_loader:\n",
    "    print(f'Text matrix size: {text_batch.size()}')\n",
    "    print(f'Target vector size: {label_batch.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim=len(vocab),\n",
    "             embedding_dim=EMBEDDING_DIM,\n",
    "             hidden_dim=HIDDEN_DIM,\n",
    "             output_dim=NUM_CLASSES # could use 1 for binary classification\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       mode='max',\n",
    "                                                       verbose=True)\n",
    "\n",
    "minibatch_loss_list, train_acc_list, valid_acc_list = train_model(\n",
    "    model=model,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    logging_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(minibatch_loss_list=minibatch_loss_list,\n",
    "                   num_epochs=NUM_EPOCHS,\n",
    "                   iter_per_epoch=len(train_loader),\n",
    "                   results_dir=None,\n",
    "                   averaging_iterations=100)\n",
    "\n",
    "plot_accuracy(train_acc_list=train_acc_list,\n",
    "              valid_acc_list=valid_acc_list,\n",
    "              results_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model, vocab, and optimizer state\n",
    "torch.save(model.state_dict(), 'model_data/lstm_02.pt')\n",
    "torch.save(vocab, 'model_data/vocab_02.pt')\n",
    "torch.save(optimizer.state_dict(), 'model_data/optimizer_02.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
