{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Training - MalReformer\n",
    "\n",
    "The tokenizer will be trained using the `google/reformer-crime-and-punishment` algorithm from the `tokenizers` library. Training data is from GMIC dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open ('../settings.json') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "DATA_DIR = os.path.join(settings['data_path'], 'classifier')\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "CHECKPOINT_DIR = os.path.join(DATA_DIR, \"model_data\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "CHECKPOINT_PREFIX = os.path.join(CHECKPOINT_DIR, \"tokenizer\")\n",
    "\n",
    "db_uri = settings['sqlalchemy_database_uri']\n",
    "\n",
    "VOCAB_SIZE = 20_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data from the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading benign reports from file\n"
     ]
    }
   ],
   "source": [
    "from utils.mal_data import get_mal_data\n",
    "benign_reports = get_mal_data(['benign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reports: 100%|██████████| 9714/9714 [00:10<00:00, 926.64it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "json_data = []\n",
    "\n",
    "# make sure that CHECKPOINT_PREFIX exists\n",
    "os.makedirs(CHECKPOINT_PREFIX, exist_ok=True)\n",
    "\n",
    "file_count = 0\n",
    "for report in tqdm(benign_reports, desc=\"Processing reports\"):\n",
    "    text = \" \".join(report[0])\n",
    "    label = 0\n",
    "\n",
    "    # add to json_data\n",
    "    json_data.append({\"text\": text, \"label\": label})\n",
    "\n",
    "    # write to file every 1000 lines\n",
    "    if len(json_data) == 1000:\n",
    "        with open(os.path.join(CHECKPOINT_PREFIX, f\"json_{file_count}.json\"), 'w') as f:\n",
    "            for line in json_data:\n",
    "                json.dump(line, f)\n",
    "                f.write(\"\\n\")\n",
    "        json_data = []\n",
    "        file_count += 1\n",
    "    \n",
    "# save last file\n",
    "if len(json_data) > 0:\n",
    "    with open(os.path.join(CHECKPOINT_PREFIX, f\"json_{file_count}.json\"), 'w') as f:\n",
    "        for line in json_data:\n",
    "            json.dump(line, f)\n",
    "            f.write(\"\\n\")\n",
    "    json_data = []\n",
    "    file_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/mike/data/gimc/classifier/model_data/tokenizer/json_0.json',\n",
       " '/media/mike/data/gimc/classifier/model_data/tokenizer/json_1.json',\n",
       " '/media/mike/data/gimc/classifier/model_data/tokenizer/json_2.json',\n",
       " '/media/mike/data/gimc/classifier/model_data/tokenizer/json_3.json',\n",
       " '/media/mike/data/gimc/classifier/model_data/tokenizer/json_4.json',\n",
       " '/media/mike/data/gimc/classifier/model_data/tokenizer/json_5.json',\n",
       " '/media/mike/data/gimc/classifier/model_data/tokenizer/json_6.json',\n",
       " '/media/mike/data/gimc/classifier/model_data/tokenizer/json_7.json',\n",
       " '/media/mike/data/gimc/classifier/model_data/tokenizer/json_8.json',\n",
       " '/media/mike/data/gimc/classifier/model_data/tokenizer/json_9.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get file names in CHECKPOINT_PREFIX\n",
    "import glob\n",
    "json_files = glob.glob(os.path.join(CHECKPOINT_PREFIX, \"*.json\"))\n",
    "json_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " heap name not found regopenkey hklm system curren\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "class LineIterator:\n",
    "    def __init__(self, file_paths):\n",
    "        \"\"\"\n",
    "        Iterator to yield lines from a file.\n",
    "\n",
    "        :param file_path: Paths to the file.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_path in self.file_paths:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    json_data = json.loads(line)\n",
    "                    yield json_data['text']\n",
    "\n",
    "# Example usage:\n",
    "file_paths = json_files\n",
    "line_iterator = LineIterator(file_paths)\n",
    "for line in line_iterator:\n",
    "    print(line[1000:1050])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"google/reformer-crime-and-punishment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example tokenization of old tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁p',\n",
       " 'ro',\n",
       " 'c',\n",
       " 'ess',\n",
       " '▁st',\n",
       " 'art',\n",
       " '▁su',\n",
       " 'c',\n",
       " 'c',\n",
       " 'ess',\n",
       " '▁th',\n",
       " 're',\n",
       " 'ad',\n",
       " '▁c',\n",
       " 're',\n",
       " 'at',\n",
       " 'e',\n",
       " '▁su',\n",
       " 'c',\n",
       " 'c',\n",
       " 'ess',\n",
       " '▁l',\n",
       " 'o',\n",
       " 'ad',\n",
       " '▁',\n",
       " 'im',\n",
       " 'a',\n",
       " 'ge',\n",
       " '▁c',\n",
       " ':',\n",
       " '▁u',\n",
       " 's',\n",
       " 'er',\n",
       " 's',\n",
       " '▁u',\n",
       " 's',\n",
       " 'er',\n",
       " '▁p',\n",
       " 'ro',\n",
       " 'j',\n",
       " 'e',\n",
       " 'ct',\n",
       " 's',\n",
       " '▁g',\n",
       " 'im',\n",
       " 'c',\n",
       " '▁s',\n",
       " 'and',\n",
       " 'b',\n",
       " 'o',\n",
       " 'x',\n",
       " '▁t',\n",
       " 'est',\n",
       " 's',\n",
       " 'am',\n",
       " 'p',\n",
       " 'le',\n",
       " '.',\n",
       " 'e',\n",
       " 'x',\n",
       " 'e',\n",
       " '▁su',\n",
       " 'c',\n",
       " 'c',\n",
       " 'ess',\n",
       " '▁l',\n",
       " 'o',\n",
       " 'ad',\n",
       " '▁',\n",
       " 'im',\n",
       " 'a',\n",
       " 'ge',\n",
       " '▁c',\n",
       " ':',\n",
       " '▁w',\n",
       " 'in',\n",
       " 'd',\n",
       " 'ow',\n",
       " 's',\n",
       " '▁s',\n",
       " 'y',\n",
       " 'st',\n",
       " 'e',\n",
       " 'm',\n",
       " '<unk>',\n",
       " '▁n',\n",
       " 't',\n",
       " 'd',\n",
       " 'll',\n",
       " '.',\n",
       " 'd',\n",
       " 'll',\n",
       " '▁su',\n",
       " 'c',\n",
       " 'c',\n",
       " 'ess',\n",
       " '▁c',\n",
       " 're',\n",
       " 'at',\n",
       " 'e',\n",
       " 'f',\n",
       " 'i',\n",
       " 'le',\n",
       " '▁c',\n",
       " ':',\n",
       " '▁w',\n",
       " 'in',\n",
       " 'd',\n",
       " 'ow',\n",
       " 's',\n",
       " '▁p',\n",
       " 're',\n",
       " 'f',\n",
       " 'et',\n",
       " 'ch',\n",
       " '▁t',\n",
       " 'est',\n",
       " 's',\n",
       " 'am',\n",
       " 'p',\n",
       " 'le',\n",
       " '.',\n",
       " 'e',\n",
       " 'x',\n",
       " 'e',\n",
       " '-',\n",
       " '<unk>',\n",
       " 'a',\n",
       " '<unk>',\n",
       " 'f',\n",
       " '.',\n",
       " 'p',\n",
       " 'f',\n",
       " '▁su',\n",
       " 'c',\n",
       " 'c',\n",
       " 'ess',\n",
       " '▁qu',\n",
       " 'er',\n",
       " 'y',\n",
       " 'st',\n",
       " 'and',\n",
       " 'ard',\n",
       " 'in',\n",
       " 'f',\n",
       " 'or',\n",
       " 'm',\n",
       " 'ation',\n",
       " 'f',\n",
       " 'i',\n",
       " 'le',\n",
       " '▁c',\n",
       " ':',\n",
       " '▁w',\n",
       " 'in',\n",
       " 'd',\n",
       " 'ow',\n",
       " 's',\n",
       " '▁p',\n",
       " 're',\n",
       " 'f',\n",
       " 'et',\n",
       " 'ch',\n",
       " '▁t',\n",
       " 'est',\n",
       " 's',\n",
       " 'am',\n",
       " 'p',\n",
       " 'le',\n",
       " '.',\n",
       " 'e',\n",
       " 'x',\n",
       " 'e',\n",
       " '-',\n",
       " '<unk>',\n",
       " 'a',\n",
       " '<unk>',\n",
       " 'f',\n",
       " '.',\n",
       " 'p',\n",
       " 'f',\n",
       " '▁su',\n",
       " 'c',\n",
       " 'c',\n",
       " 'ess',\n",
       " '▁re',\n",
       " 'ad',\n",
       " 'f',\n",
       " 'i',\n",
       " 'le',\n",
       " '▁c',\n",
       " ':',\n",
       " '▁w',\n",
       " 'in',\n",
       " 'd',\n",
       " 'ow',\n",
       " 's',\n",
       " '▁p',\n",
       " 're',\n",
       " 'f']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(line_iterator)\n",
    "tokens = old_tokenizer.tokenize(next(it))\n",
    "tokens[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(line_iterator, VOCAB_SIZE, min_frequency=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example tokenization of new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁process',\n",
       " '▁start',\n",
       " '▁success',\n",
       " '▁thread',\n",
       " '▁create',\n",
       " '▁success',\n",
       " '▁load',\n",
       " '▁image',\n",
       " '▁c:',\n",
       " '▁users',\n",
       " '▁user',\n",
       " '▁projects',\n",
       " '▁gimc',\n",
       " '▁sandbox',\n",
       " '▁testsample.exe',\n",
       " '▁success',\n",
       " '▁load',\n",
       " '▁image',\n",
       " '▁c:',\n",
       " '▁windows',\n",
       " '▁system32',\n",
       " '▁ntdll.dll',\n",
       " '▁success',\n",
       " '▁createfile',\n",
       " '▁c:',\n",
       " '▁windows',\n",
       " '▁prefetch',\n",
       " '▁testsample.exe-37254a4f.pf',\n",
       " '▁success',\n",
       " '▁querystandardinformationfile',\n",
       " '▁c:',\n",
       " '▁windows',\n",
       " '▁prefetch',\n",
       " '▁testsample.exe-37254a4f.pf',\n",
       " '▁success',\n",
       " '▁readfile',\n",
       " '▁c:',\n",
       " '▁windows',\n",
       " '▁prefetch',\n",
       " '▁testsample.exe-37254a4f.pf',\n",
       " '▁success',\n",
       " '▁readfile',\n",
       " '▁c:',\n",
       " '▁windows',\n",
       " '▁prefetch',\n",
       " '▁testsample.exe-37254a4f.pf',\n",
       " '▁success',\n",
       " '▁closefile',\n",
       " '▁c:',\n",
       " '▁windows',\n",
       " '▁prefetch',\n",
       " '▁testsample.exe-37254a4f.pf',\n",
       " '▁success',\n",
       " '▁regopenkey',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁session',\n",
       " '▁manager',\n",
       " '▁reparse',\n",
       " '▁regopenkey',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁session',\n",
       " '▁manager',\n",
       " '▁success',\n",
       " '▁regqueryvalue',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁session',\n",
       " '▁manager',\n",
       " '▁raiseexceptiononpossibledeadlock',\n",
       " '▁name',\n",
       " '▁not',\n",
       " '▁found',\n",
       " '▁regclosekey',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁session',\n",
       " '▁manager',\n",
       " '▁success',\n",
       " '▁regopenkey',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁session',\n",
       " '▁manager',\n",
       " '▁segment',\n",
       " '▁heap',\n",
       " '▁reparse',\n",
       " '▁regopenkey',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁session',\n",
       " '▁manager',\n",
       " '▁segment',\n",
       " '▁heap',\n",
       " '▁name',\n",
       " '▁not',\n",
       " '▁found',\n",
       " '▁regopenkey',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁session',\n",
       " '▁manager',\n",
       " '▁reparse',\n",
       " '▁regopenkey',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁session',\n",
       " '▁manager',\n",
       " '▁success',\n",
       " '▁regqueryvalue',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁session',\n",
       " '▁manager',\n",
       " '▁resourcepolicies',\n",
       " '▁name',\n",
       " '▁not',\n",
       " '▁found',\n",
       " '▁regclosekey',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁session',\n",
       " '▁manager',\n",
       " '▁success',\n",
       " '▁createfile',\n",
       " '▁c:',\n",
       " '▁users',\n",
       " '▁user',\n",
       " '▁projects',\n",
       " '▁gimc',\n",
       " '▁sandbox',\n",
       " '▁success',\n",
       " '▁load',\n",
       " '▁image',\n",
       " '▁c:',\n",
       " '▁windows',\n",
       " '▁system32',\n",
       " '▁kernel32.dll',\n",
       " '▁success',\n",
       " '▁load',\n",
       " '▁image',\n",
       " '▁c:',\n",
       " '▁windows',\n",
       " '▁system32',\n",
       " '▁kernelbase.dll',\n",
       " '▁success',\n",
       " '▁regopenkey',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁stateseparation',\n",
       " '▁redirectionmap',\n",
       " '▁keys',\n",
       " '▁reparse',\n",
       " '▁regopenkey',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁stateseparation',\n",
       " '▁redirectionmap',\n",
       " '▁keys',\n",
       " '▁name',\n",
       " '▁not',\n",
       " '▁found',\n",
       " '▁regopenkey',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control',\n",
       " '▁safeboot',\n",
       " '▁option',\n",
       " '▁reparse',\n",
       " '▁regopenkey',\n",
       " '▁hklm',\n",
       " '▁system',\n",
       " '▁currentcontrolset',\n",
       " '▁control']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(line_iterator)\n",
    "tokens = tokenizer.tokenize(next(it))\n",
    "tokens[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/media/mike/data/gimc/classifier/model_data/mal-reformer/tokenizer_config.json',\n",
       " '/media/mike/data/gimc/classifier/model_data/mal-reformer/special_tokens_map.json',\n",
       " '/media/mike/data/gimc/classifier/model_data/mal-reformer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_path = os.path.join(CHECKPOINT_DIR, \"mal-reformer\")\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
