{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277ca748",
   "metadata": {},
   "source": [
    "## Setup - Import modules and setup database connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219de2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules and setup database connection\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "\n",
    "# Load settings from settings.json\n",
    "settings_file = '../settings.json'\n",
    "with open(settings_file) as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "# Database setup\n",
    "DATABASE_URL = settings['sqlalchemy_database_uri']\n",
    "engine = create_engine(DATABASE_URL, echo=False)\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "# move current directory to parent directory\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "from models import Sample, Tag, Analysis, sample_tag, Prototypes, Ingredient, Candidate\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f4a15",
   "metadata": {},
   "source": [
    "## Count all major entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1621c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the database: 5931\n",
      "Number of tags in the database: 10\n",
      "Number of analyses in the database: 17865\n",
      "Number of prototypes in the database: 0\n",
      "Number of ingredients in the database: 0\n",
      "Number of candidates in the database: 6\n"
     ]
    }
   ],
   "source": [
    "# counts of all the major entities\n",
    "session.expire_all()\n",
    "\n",
    "count_samples = session.query(Sample).count()\n",
    "count_Tags = session.query(Tag).count()\n",
    "count_Analyses = session.query(Analysis).count()\n",
    "count_Prototypes = session.query(Prototypes).count()\n",
    "count_Ingredients = session.query(Ingredient).count()\n",
    "count_Candidates = session.query(Candidate).count()\n",
    "print(f\"Number of samples in the database: {count_samples}\")\n",
    "print(f\"Number of tags in the database: {count_Tags}\")\n",
    "print(f\"Number of analyses in the database: {count_Analyses}\")\n",
    "print(f\"Number of prototypes in the database: {count_Prototypes}\")\n",
    "print(f\"Number of ingredients in the database: {count_Ingredients}\")\n",
    "print(f\"Number of candidates in the database: {count_Candidates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f182c",
   "metadata": {},
   "source": [
    "## Get tags and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5030964b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tag Sample Counts:\n",
      "Tag: ('family', 'bsi'), Sample Count: 3\n",
      "Tag: ('tatic', 'scheduled_task'), Sample Count: 3\n",
      "Tag: ('ttp', 'wmi'), Sample Count: 1\n",
      "Tag: ('ttp', 'com'), Sample Count: 1\n",
      "Tag: ('ttp', 'cmd'), Sample Count: 1\n",
      "Tag: ('family', 'benign'), Sample Count: 5908\n",
      "Tag: ('class', 'wmi'), Sample Count: 14\n",
      "Tag: ('class', 'purple'), Sample Count: 1\n",
      "Tag: ('class', 'cow'), Sample Count: 1\n",
      "Tag: ('class', 'com'), Sample Count: 1\n"
     ]
    }
   ],
   "source": [
    "# get tags and their associated sample counts\n",
    "session.expire_all()\n",
    "\n",
    "tags = session.query(Tag).all()\n",
    "tag_sample_counts = {}\n",
    "for tag in tags:\n",
    "    sample_count = session.query(Sample).join(sample_tag).filter(sample_tag.c.tag_id == tag.id).count()\n",
    "    tag_sample_counts[(tag.key, tag.value)] = sample_count\n",
    "print(\"\\nTag Sample Counts:\")\n",
    "for tag_value, sample_count in tag_sample_counts.items():\n",
    "    print(f\"Tag: {tag_value}, Sample Count: {sample_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10a3128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples with tag (class=wmi):\n",
      "Sample ID: 3e538fdd57f368e3cbd31c14a9e1e6880c81e94c93282871c903020471a14190, Name: /mnt/data/gimc/3e/3e53/3e538fdd57f368e3cbd31c14a9e1e6880c81e94c93282871c903020471a14190\n",
      "Sample ID: f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191, Name: /mnt/data/gimc/f2/f2a8/f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191\n",
      "Sample ID: b788d12fed5dd6ad3c0331ee21e8b4c7f568b7f38116457a83370875a1315971, Name: /mnt/data/gimc/b7/b788/b788d12fed5dd6ad3c0331ee21e8b4c7f568b7f38116457a83370875a1315971\n",
      "Sample ID: 79a9409ff29b2e967161e01a0f27bcb0153a66a604e667120e30a8c09ca8deef, Name: /mnt/data/gimc/79/79a9/79a9409ff29b2e967161e01a0f27bcb0153a66a604e667120e30a8c09ca8deef\n",
      "Sample ID: c38bf4cb95005533dd52991b059bfbc60a13f81590e04735bd8a5ace221ee14b, Name: /mnt/data/gimc/c3/c38b/c38bf4cb95005533dd52991b059bfbc60a13f81590e04735bd8a5ace221ee14b\n",
      "Sample ID: 6a181382dbbf14cdab0262153bf0bcc85957f95b8d720ebe93295fe520b7cdd1, Name: /mnt/data/gimc/6a/6a18/6a181382dbbf14cdab0262153bf0bcc85957f95b8d720ebe93295fe520b7cdd1\n",
      "Sample ID: 49fa19821dc17169120ff0160580ba0053ac882f263e5d94a5fa9dd26bb1eba3, Name: /mnt/data/gimc/49/49fa/49fa19821dc17169120ff0160580ba0053ac882f263e5d94a5fa9dd26bb1eba3\n",
      "Sample ID: d078cb881b19d2e9ed54fe04985cf59f153ea14d33b20668817d44488915c019, Name: /mnt/data/gimc/d0/d078/d078cb881b19d2e9ed54fe04985cf59f153ea14d33b20668817d44488915c019\n",
      "Sample ID: 3d4049c241b839a3bb2761f634031ae53f4d7dcf7735cf035db036a3fb6cec8d, Name: /mnt/data/gimc/3d/3d40/3d4049c241b839a3bb2761f634031ae53f4d7dcf7735cf035db036a3fb6cec8d\n",
      "Sample ID: d7ba362f09d27820cee2a6a05140d02ee89b1c362c5378c41d0931a35341238e, Name: /mnt/data/gimc/d7/d7ba/d7ba362f09d27820cee2a6a05140d02ee89b1c362c5378c41d0931a35341238e\n",
      "Sample ID: ba3365452639c40e6c18255896f5d10d12d18335d7be2e1610d0b37b7575e944, Name: /mnt/data/gimc/ba/ba33/ba3365452639c40e6c18255896f5d10d12d18335d7be2e1610d0b37b7575e944\n",
      "Sample ID: 33afdf087ac21472ea67b68d1a7038ad5e170a4fc5561d4affee2a5066c7ee72, Name: /mnt/data/gimc/33/33af/33afdf087ac21472ea67b68d1a7038ad5e170a4fc5561d4affee2a5066c7ee72\n",
      "Sample ID: 2e5ce94e324f3cda8e0da23f9b71387f7c4f13793c22eb9df180e61946c425f1, Name: /mnt/data/gimc/2e/2e5c/2e5ce94e324f3cda8e0da23f9b71387f7c4f13793c22eb9df180e61946c425f1\n",
      "Sample ID: 08e482398578949fd9a23e8a7f8090a225d4801a738eb6de2dae083a3c4bff58, Name: /mnt/data/gimc/08/08e4/08e482398578949fd9a23e8a7f8090a225d4801a738eb6de2dae083a3c4bff58\n"
     ]
    }
   ],
   "source": [
    "# get all samples asscoiated with a specific tag\n",
    "session.expire_all()\n",
    "\n",
    "tag_key = 'class'\n",
    "tag_value = 'wmi'\n",
    "samples_with_tag = session.query(Sample).join(sample_tag).join(Tag).filter(Tag.key == tag_key, Tag.value == tag_value).all()\n",
    "print(f\"\\nSamples with tag ({tag_key}={tag_value}):\")\n",
    "for sample in samples_with_tag:\n",
    "    print(f\"Sample ID: {sample.sha256}, Name: {sample.filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7651b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyses with tag (class=wmi):\n",
      "Analysis ID: 17848, Sample ID: f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191, Analysis Status: 2\n",
      "Analysis ID: 17849, Sample ID: b788d12fed5dd6ad3c0331ee21e8b4c7f568b7f38116457a83370875a1315971, Analysis Status: 2\n",
      "Analysis ID: 17859, Sample ID: 49fa19821dc17169120ff0160580ba0053ac882f263e5d94a5fa9dd26bb1eba3, Analysis Status: 2\n",
      "Analysis ID: 17853, Sample ID: 79a9409ff29b2e967161e01a0f27bcb0153a66a604e667120e30a8c09ca8deef, Analysis Status: 2\n",
      "Analysis ID: 17860, Sample ID: d078cb881b19d2e9ed54fe04985cf59f153ea14d33b20668817d44488915c019, Analysis Status: 2\n",
      "Analysis ID: 17854, Sample ID: c38bf4cb95005533dd52991b059bfbc60a13f81590e04735bd8a5ace221ee14b, Analysis Status: 2\n",
      "Analysis ID: 17865, Sample ID: 2e5ce94e324f3cda8e0da23f9b71387f7c4f13793c22eb9df180e61946c425f1, Analysis Status: 2\n",
      "Analysis ID: 17856, Sample ID: 6a181382dbbf14cdab0262153bf0bcc85957f95b8d720ebe93295fe520b7cdd1, Analysis Status: 2\n",
      "Analysis ID: 17861, Sample ID: 3d4049c241b839a3bb2761f634031ae53f4d7dcf7735cf035db036a3fb6cec8d, Analysis Status: 2\n",
      "Analysis ID: 17862, Sample ID: d7ba362f09d27820cee2a6a05140d02ee89b1c362c5378c41d0931a35341238e, Analysis Status: 2\n",
      "Analysis ID: 17866, Sample ID: 08e482398578949fd9a23e8a7f8090a225d4801a738eb6de2dae083a3c4bff58, Analysis Status: 2\n",
      "Analysis ID: 17863, Sample ID: ba3365452639c40e6c18255896f5d10d12d18335d7be2e1610d0b37b7575e944, Analysis Status: 2\n",
      "Analysis ID: 17864, Sample ID: 33afdf087ac21472ea67b68d1a7038ad5e170a4fc5561d4affee2a5066c7ee72, Analysis Status: 2\n"
     ]
    }
   ],
   "source": [
    "# get all analyses and with a specific tag\n",
    "session.expire_all()\n",
    "\n",
    "tag_key = 'class'\n",
    "tag_value = 'wmi'\n",
    "analyses_with_tag = session.query(Analysis).join(Sample).join(sample_tag).join(Tag).filter(Tag.key == tag_key, Tag.value == tag_value).all()\n",
    "print(f\"\\nAnalyses with tag ({tag_key}={tag_value}):\")\n",
    "for analysis in analyses_with_tag:\n",
    "    print(f\"Analysis ID: {analysis.id}, Sample ID: {analysis.sample}, Analysis Status: {analysis.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3868b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Status Counts:\n",
      "Status: 2, Count: 17318\n",
      "Status: 3, Count: 547\n"
     ]
    }
   ],
   "source": [
    "# get counts of all statuses of analyses\n",
    "session.expire_all()\n",
    "\n",
    "from sqlalchemy import func\n",
    "analysis_status_counts = session.query(Analysis.status, func.count(Analysis.id)).group_by(Analysis.status).all()\n",
    "print(\"\\nAnalysis Status Counts:\")\n",
    "for status, count in analysis_status_counts:\n",
    "    print(f\"Status: {status}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4ff7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyses for sample with SHA256 f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191:\n",
      "Analysis ID: 17848, Status: 2\n"
     ]
    }
   ],
   "source": [
    "# get analysis by sample sha256\n",
    "session.expire_all()\n",
    "\n",
    "sample_sha256 = 'f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191'\n",
    "analysis_for_sample = session.query(Analysis).join(Sample).filter(Sample.sha256 == sample_sha256).all()\n",
    "print(f\"\\nAnalyses for sample with SHA256 {sample_sha256}:\")\n",
    "for analysis in analysis_for_sample:\n",
    "    print(f\"Analysis ID: {analysis.id}, Status: {analysis.status}\")\n",
    "if not analysis_for_sample:\n",
    "    print(\"No analyses found for this sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05105727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis ID: 17849, Status: 2, Sample ID: b788d12fed5dd6ad3c0331ee21e8b4c7f568b7f38116457a83370875a1315971\n"
     ]
    }
   ],
   "source": [
    "# get analysis by its ID\n",
    "session.expire_all()\n",
    "\n",
    "analysis_id = 17849  # replace with desired analysis ID\n",
    "analysis = session.query(Analysis).filter(Analysis.id == analysis_id).first()\n",
    "if analysis:\n",
    "    print(f\"\\nAnalysis ID: {analysis.id}, Status: {analysis.status}, Sample ID: {analysis.sample}\")\n",
    "else:\n",
    "    print(f\"\\nNo analysis found with ID {analysis_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd8cfc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidates:\n",
      "Candidate ID: 7abec6d0e8b676015f472d274491b5a8d5ee0339ea4b53aa55c6a7e3694db9ee, Status: 3, F1: 0.2, F2: 0.3333333333333333, F3: 0.02028191275894642, Analysis ID: 17861, Error: None\n",
      "Candidate ID: 95bb77e5e87811eb21cd17dcdddbcda388125ba3da7d5b6df5860eac732ae09b, Status: 3, F1: 0.14285714285714285, F2: 0.0, F3: 0.0, Analysis ID: None, Error: Compilation failed: 2 errors, 0 warnings\n",
      "Candidate ID: baa9f40b8d15fd64663654eb3c7f299fb33b904d6a8d3fd0918fbef8a162f86b, Status: 3, F1: 1.0, F2: 1.0, F3: 0.9153369069099426, Analysis ID: 17871, Error: None\n",
      "Candidate ID: a29097919f40ba277abda67cdf616d59eda96649b3b2d8d50a8b4410c642ffc6, Status: 3, F1: 1.0, F2: 0.3333333333333333, F3: 0.02028191275894642, Analysis ID: 17862, Error: None\n",
      "Candidate ID: 76b6cc1637d9e65dae68fd71f00883652bd7914f47016f7189fb451ec08e187e, Status: 3, F1: 0.5, F2: 1.0, F3: 0.989303469657898, Analysis ID: 17863, Error: Build VM timeout\n",
      "Candidate ID: a4e381fdab1f71481eb33e09e9528800234c7da84a6d30f63f3339b20c03e71b, Status: 3, F1: 1.0, F2: 1.0, F3: 0.989303469657898, Analysis ID: 17866, Error: None\n",
      "Candidate ID: 67f5f2678acff690d5dc4c5104edf22332242799037cf535de8f8654aaa43d16, Status: 3, F1: 1.0, F2: 0.0, F3: 0.9981977343559265, Analysis ID: 17869, Error: None\n",
      "Candidate ID: 33b2547820326675e8356ec6cad137b52e1b57990e66e180b4089356c7753b43, Status: 3, F1: 1.0, F2: 1.0, F3: 0.9968795776367188, Analysis ID: 17870, Error: None\n"
     ]
    }
   ],
   "source": [
    "# get candidates\n",
    "session.expire_all()\n",
    "\n",
    "candidates = session.query(Candidate).all()\n",
    "print(\"\\nCandidates:\")\n",
    "for candidate in candidates:\n",
    "    print(f\"Candidate ID: {candidate.hash}, Status: {candidate.status}, F1: {candidate.F1}, F2: {candidate.F2}, F3: {candidate.F3}, Analysis ID: {candidate.analysis_id}, Error: {candidate.error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4921f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated candidate 67f5f2678acff690d5dc4c5104edf22332242799037cf535de8f8654aaa43d16 to status '0'.\n"
     ]
    }
   ],
   "source": [
    "# # update status of a candidate\n",
    "# session.expire_all()\n",
    "\n",
    "# candidate_hash = '67f5f2678acff690d5dc4c5104edf22332242799037cf535de8f8654aaa43d16'  # replace with actual candidate hash\n",
    "# candidate = session.query(Candidate).filter(Candidate.hash == candidate_hash).first()\n",
    "# if candidate:\n",
    "#     candidate.status = 0  # replace with desired status\n",
    "#     session.commit()\n",
    "#     print(f\"\\nUpdated candidate {candidate_hash} to status '{candidate.status}'.\")\n",
    "# else:\n",
    "#     print(f\"\\nNo candidate found with hash {candidate_hash}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef331e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get full candidate details\n",
    "session.expire_all()\n",
    "\n",
    "import base64\n",
    "candidate_hash = 'a4e381fdab1f71481eb33e09e9528800234c7da84a6d30f63f3339b20c03e71b'  # replace with actual candidate hash\n",
    "candidate = session.query(Candidate).filter(Candidate.hash == candidate_hash).first()\n",
    "if candidate:\n",
    "    print(f\"\\nCandidate Details for {candidate_hash}:\")\n",
    "    print(f\"Hash: {candidate.hash}\")\n",
    "    print(f\"Status: {candidate.status}\")\n",
    "    print(f\"F1: {candidate.F1}\")\n",
    "    print(f\"F2: {candidate.F2}\")\n",
    "    print(f\"F3: {candidate.F3}\")\n",
    "    print(f\"Analysis ID: {candidate.analysis_id}\")\n",
    "    # unbase64 the code snippet and print first XX characters\n",
    "    decoded_code = base64.b64decode(candidate.code).decode('utf-8', errors='ignore')\n",
    "    print(f\"Code Snippet: {decoded_code[:500]}\")\n",
    "else:\n",
    "    print(f\"\\nNo candidate found with hash {candidate_hash}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daaa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoded_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample from analysis ID\n",
    "session.expire_all()\n",
    "\n",
    "analysis_id = 5000  # replace with desired analysis ID\n",
    "analysis = session.query(Analysis).filter(Analysis.id == analysis_id).first()\n",
    "if analysis:\n",
    "    sample = session.query(Sample).filter(Sample.sha256 == analysis.sample).first()\n",
    "    if sample:\n",
    "        print(f\"\\nSample for Analysis ID {analysis_id}:\")\n",
    "        print(f\"Sample ID: {sample.sha256}, Name: {sample.filepath}\")\n",
    "    else:\n",
    "        print(f\"\\nNo sample found for Analysis ID {analysis_id}.\")\n",
    "    # get all tags for the sample\n",
    "    tags = session.query(Tag).join(sample_tag).join(Sample).filter(Sample.sha256 == sample.sha256).all()\n",
    "    print(f\"\\nTags for Sample ID {sample.sha256}:\")\n",
    "    for tag in tags:\n",
    "        print(f\"Tag: {tag.key}={tag.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from classifier.models.cnn_nlp import CNN_NLP\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "from transformers import AutoTokenizer\n",
    "classifier_path = '/mnt/data/gimc/classifier/model_data/cnn4bsi_checkpoint.pth'\n",
    "tokenizer_path = '/mnt/data/gimc/classifier/model_data/mal-reformer'\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "TOKENIZER.pad_token = \"[PAD]\"\n",
    "TOKENIZER.cls_token = \"[CLS]\"\n",
    "TOKENIZER.sep_token = \"[SEP]\"\n",
    "\n",
    "checkpoint = torch.load(classifier_path)\n",
    "vocab_size = 20000\n",
    "embed_dim = 128\n",
    "num_classes = 4\n",
    "dropout = 0.5\n",
    "MODEL = CNN_NLP(\n",
    "    pretrained_embedding=None,\n",
    "    freeze_embedding=False,\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    num_filters=[10, 10, 10],\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout\n",
    ")\n",
    "MODEL.load_state_dict(checkpoint['model_states'][-1])\n",
    "MODEL.to(DEVICE)\n",
    "MODEL.eval()\n",
    "\n",
    "def mal_tokenizer(line):\n",
    "    \"\"\"\n",
    "    Tokenize a line of text\n",
    "    \"\"\"\n",
    "    line = line.lower()\n",
    "    line = line.replace(',', ' ')\n",
    "    line = line.replace('\\\\', ' ')\n",
    "    line = line.replace('\\\\\\\\', ' ')\n",
    "    return line.split()\n",
    "\n",
    "dynamic_report_tokenized = []\n",
    "\n",
    "with open(analysis.report) as f:\n",
    "    report = f.read()\n",
    "    dynamic_report = json.loads(report)['dynamic']\n",
    "    for item in dynamic_report:\n",
    "        line = f\"{item['Operation']}, {item['Path']}, {item['Result']}\"\n",
    "        dynamic_report_tokenized.extend(mal_tokenizer(line))\n",
    "\n",
    "report_text = \" \".join(dynamic_report_tokenized)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20480 * 2\n",
    "inputs = TOKENIZER(\n",
    "    report_text,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    return_tensors='pt'\n",
    ").to(DEVICE)\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    logits = MODEL(input_ids)\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "# turn props into a list of probabilities\n",
    "probabilities = probs.cpu().numpy().flatten().tolist()\n",
    "print(\"Class Probabilities:\")\n",
    "for i, prob in enumerate(probabilities):\n",
    "    print(f\"Class {i}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
