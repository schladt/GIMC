{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277ca748",
   "metadata": {},
   "source": [
    "## Setup - Import modules and setup database connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219de2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules and setup database connection\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "\n",
    "# Load settings from settings.json\n",
    "settings_file = '../settings.json'\n",
    "with open(settings_file) as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "# Database setup\n",
    "DATABASE_URL = settings['sqlalchemy_database_uri']\n",
    "engine = create_engine(DATABASE_URL, echo=False)\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "# move current directory to parent directory\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "from sandbox.models import Sample, Tag, Analysis, sample_tag\n",
    "from gi.models import Prototypes, Ingredient, Candidate\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f4a15",
   "metadata": {},
   "source": [
    "## Count all major entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1621c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the database: 5917\n",
      "Number of tags in the database: 7\n",
      "Number of analyses in the database: 17851\n",
      "Number of prototypes in the database: 0\n",
      "Number of ingredients in the database: 0\n",
      "Number of candidates in the database: 1\n"
     ]
    }
   ],
   "source": [
    "# counts of all the major entities\n",
    "session.expire_all()\n",
    "\n",
    "count_samples = session.query(Sample).count()\n",
    "count_Tags = session.query(Tag).count()\n",
    "count_Analyses = session.query(Analysis).count()\n",
    "count_Prototypes = session.query(Prototypes).count()\n",
    "count_Ingredients = session.query(Ingredient).count()\n",
    "count_Candidates = session.query(Candidate).count()\n",
    "print(f\"Number of samples in the database: {count_samples}\")\n",
    "print(f\"Number of tags in the database: {count_Tags}\")\n",
    "print(f\"Number of analyses in the database: {count_Analyses}\")\n",
    "print(f\"Number of prototypes in the database: {count_Prototypes}\")\n",
    "print(f\"Number of ingredients in the database: {count_Ingredients}\")\n",
    "print(f\"Number of candidates in the database: {count_Candidates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f182c",
   "metadata": {},
   "source": [
    "## Get tags and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5030964b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tag Sample Counts:\n",
      "Tag: ('family', 'bsi'), Sample Count: 3\n",
      "Tag: ('tatic', 'scheduled_task'), Sample Count: 3\n",
      "Tag: ('ttp', 'wmi'), Sample Count: 1\n",
      "Tag: ('ttp', 'com'), Sample Count: 1\n",
      "Tag: ('ttp', 'cmd'), Sample Count: 1\n",
      "Tag: ('family', 'benign'), Sample Count: 5908\n",
      "Tag: ('class', 'wmi'), Sample Count: 3\n"
     ]
    }
   ],
   "source": [
    "# get tags and their associated sample counts\n",
    "session.expire_all()\n",
    "\n",
    "tags = session.query(Tag).all()\n",
    "tag_sample_counts = {}\n",
    "for tag in tags:\n",
    "    sample_count = session.query(Sample).join(sample_tag).filter(sample_tag.c.tag_id == tag.id).count()\n",
    "    tag_sample_counts[(tag.key, tag.value)] = sample_count\n",
    "print(\"\\nTag Sample Counts:\")\n",
    "for tag_value, sample_count in tag_sample_counts.items():\n",
    "    print(f\"Tag: {tag_value}, Sample Count: {sample_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10a3128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples with tag (class=wmi):\n",
      "Sample ID: 3e538fdd57f368e3cbd31c14a9e1e6880c81e94c93282871c903020471a14190, Name: /mnt/data/gimc/3e/3e53/3e538fdd57f368e3cbd31c14a9e1e6880c81e94c93282871c903020471a14190\n",
      "Sample ID: f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191, Name: /mnt/data/gimc/f2/f2a8/f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191\n",
      "Sample ID: b788d12fed5dd6ad3c0331ee21e8b4c7f568b7f38116457a83370875a1315971, Name: /mnt/data/gimc/b7/b788/b788d12fed5dd6ad3c0331ee21e8b4c7f568b7f38116457a83370875a1315971\n"
     ]
    }
   ],
   "source": [
    "# get all samples asscoiated with a specific tag\n",
    "session.expire_all()\n",
    "\n",
    "tag_key = 'class'\n",
    "tag_value = 'wmi'\n",
    "samples_with_tag = session.query(Sample).join(sample_tag).join(Tag).filter(Tag.key == tag_key, Tag.value == tag_value).all()\n",
    "print(f\"\\nSamples with tag ({tag_key}={tag_value}):\")\n",
    "for sample in samples_with_tag:\n",
    "    print(f\"Sample ID: {sample.sha256}, Name: {sample.filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7651b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyses with tag (class=wmi):\n",
      "Analysis ID: 17848, Sample ID: f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191, Analysis Status: 2\n",
      "Analysis ID: 17849, Sample ID: b788d12fed5dd6ad3c0331ee21e8b4c7f568b7f38116457a83370875a1315971, Analysis Status: 2\n"
     ]
    }
   ],
   "source": [
    "# get all analyses and with a specific tag\n",
    "session.expire_all()\n",
    "\n",
    "tag_key = 'class'\n",
    "tag_value = 'wmi'\n",
    "analyses_with_tag = session.query(Analysis).join(Sample).join(sample_tag).join(Tag).filter(Tag.key == tag_key, Tag.value == tag_value).all()\n",
    "print(f\"\\nAnalyses with tag ({tag_key}={tag_value}):\")\n",
    "for analysis in analyses_with_tag:\n",
    "    print(f\"Analysis ID: {analysis.id}, Sample ID: {analysis.sample}, Analysis Status: {analysis.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3868b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Status Counts:\n",
      "Status: 0, Count: 2\n",
      "Status: 3, Count: 547\n",
      "Status: 2, Count: 17302\n"
     ]
    }
   ],
   "source": [
    "# get counts of all statuses of analyses\n",
    "session.expire_all()\n",
    "\n",
    "from sqlalchemy import func\n",
    "analysis_status_counts = session.query(Analysis.status, func.count(Analysis.id)).group_by(Analysis.status).all()\n",
    "print(\"\\nAnalysis Status Counts:\")\n",
    "for status, count in analysis_status_counts:\n",
    "    print(f\"Status: {status}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4ff7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyses for sample with SHA256 f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191:\n",
      "Analysis ID: 17848, Status: 2\n"
     ]
    }
   ],
   "source": [
    "# get analysis by sample sha256\n",
    "session.expire_all()\n",
    "\n",
    "sample_sha256 = 'f2a839f3eac858ddb450a162a9faa6fe54391fb0f0b0c715584cffe36db6e191'\n",
    "analysis_for_sample = session.query(Analysis).join(Sample).filter(Sample.sha256 == sample_sha256).all()\n",
    "print(f\"\\nAnalyses for sample with SHA256 {sample_sha256}:\")\n",
    "for analysis in analysis_for_sample:\n",
    "    print(f\"Analysis ID: {analysis.id}, Status: {analysis.status}\")\n",
    "if not analysis_for_sample:\n",
    "    print(\"No analyses found for this sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05105727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis ID: 17849, Status: 2, Sample ID: b788d12fed5dd6ad3c0331ee21e8b4c7f568b7f38116457a83370875a1315971\n"
     ]
    }
   ],
   "source": [
    "# get analysis by its ID\n",
    "session.expire_all()\n",
    "\n",
    "analysis_id = 17849  # replace with desired analysis ID\n",
    "analysis = session.query(Analysis).filter(Analysis.id == analysis_id).first()\n",
    "if analysis:\n",
    "    print(f\"\\nAnalysis ID: {analysis.id}, Status: {analysis.status}, Sample ID: {analysis.sample}\")\n",
    "else:\n",
    "    print(f\"\\nNo analysis found with ID {analysis_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd8cfc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidates:\n",
      "Candidate ID: 76b6cc1637d9e65dae68fd71f00883652bd7914f47016f7189fb451ec08e187e, Status: 0, F1: 1.0, F2: 1.0, F3: 0.0, Analysis ID: 17853\n"
     ]
    }
   ],
   "source": [
    "# get candidates\n",
    "session.expire_all()\n",
    "\n",
    "candidates = session.query(Candidate).all()\n",
    "print(\"\\nCandidates:\")\n",
    "for candidate in candidates:\n",
    "    print(f\"Candidate ID: {candidate.hash}, Status: {candidate.status}, F1: {candidate.F1}, F2: {candidate.F2}, F3: {candidate.F3}, Analysis ID: {candidate.analysis_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4921f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated candidate 76b6cc1637d9e65dae68fd71f00883652bd7914f47016f7189fb451ec08e187e to status '0'.\n"
     ]
    }
   ],
   "source": [
    "# # update status of a candidate\n",
    "# session.expire_all()\n",
    "\n",
    "# candidate_hash = '76b6cc1637d9e65dae68fd71f00883652bd7914f47016f7189fb451ec08e187e'  # replace with actual candidate hash\n",
    "# candidate = session.query(Candidate).filter(Candidate.hash == candidate_hash).first()\n",
    "# if candidate:\n",
    "#     candidate.status = 0  # replace with desired status\n",
    "#     session.commit()\n",
    "#     print(f\"\\nUpdated candidate {candidate_hash} to status '{candidate.status}'.\")\n",
    "# else:\n",
    "#     print(f\"\\nNo candidate found with hash {candidate_hash}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef331e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Details for 76b6cc1637d9e65dae68fd71f00883652bd7914f47016f7189fb451ec08e187e:\n",
      "Hash: 76b6cc1637d9e65dae68fd71f00883652bd7914f47016f7189fb451ec08e187e\n",
      "Status: 4\n",
      "F1: 1.0\n",
      "F2: 1.0\n",
      "F3: 0.0\n",
      "Analysis ID: 17857\n",
      "Code Snippet: /*\n",
      " * WMI Event Subscription Persistence BSI (Behavioral Subset Implementation)\n",
      " * \n",
      " * This is a sanitized implementation derived from WMIGhost malware that demonstrates\n",
      " * WMI Event Subscription persistence (MITRE ATT&CK T1546.003).\n",
      " * \n",
      " * Behavior:\n",
      " * - Creates WMI ActiveScriptEventConsumer with benign payload\n",
      " * - Sets up timer-based event filter (2 second interval)\n",
      " * - Binds filter to consumer for persistence across reboots\n",
      " * \n",
      " * Payload: Writes timestamps to log file to demonstrate persis\n"
     ]
    }
   ],
   "source": [
    "# get full candidate details\n",
    "session.expire_all()\n",
    "\n",
    "import base64\n",
    "candidate_hash = '76b6cc1637d9e65dae68fd71f00883652bd7914f47016f7189fb451ec08e187e'  # replace with actual candidate hash\n",
    "candidate = session.query(Candidate).filter(Candidate.hash == candidate_hash).first()\n",
    "if candidate:\n",
    "    print(f\"\\nCandidate Details for {candidate_hash}:\")\n",
    "    print(f\"Hash: {candidate.hash}\")\n",
    "    print(f\"Status: {candidate.status}\")\n",
    "    print(f\"F1: {candidate.F1}\")\n",
    "    print(f\"F2: {candidate.F2}\")\n",
    "    print(f\"F3: {candidate.F3}\")\n",
    "    print(f\"Analysis ID: {candidate.analysis_id}\")\n",
    "    # unbase64 the code snippet and print first XX characters\n",
    "    decoded_code = base64.b64decode(candidate.code).decode('utf-8', errors='ignore')\n",
    "    print(f\"Code Snippet: {decoded_code[:500]}\")\n",
    "else:\n",
    "    print(f\"\\nNo candidate found with hash {candidate_hash}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ddb4bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample for Analysis ID 5000:\n",
      "Sample ID: 9417a8191a87057481807e31fe64079c63d6468c3bd6b3d6aae0de3fac8f3baa, Name: /mnt/data/gimc/94/9417/9417a8191a87057481807e31fe64079c63d6468c3bd6b3d6aae0de3fac8f3baa\n",
      "\n",
      "Tags for Sample ID 9417a8191a87057481807e31fe64079c63d6468c3bd6b3d6aae0de3fac8f3baa:\n",
      "Tag: family=bsi\n",
      "Tag: tatic=scheduled_task\n",
      "Tag: ttp=com\n"
     ]
    }
   ],
   "source": [
    "# get sample from analysis ID\n",
    "session.expire_all()\n",
    "\n",
    "analysis_id = 5000  # replace with desired analysis ID\n",
    "analysis = session.query(Analysis).filter(Analysis.id == analysis_id).first()\n",
    "if analysis:\n",
    "    sample = session.query(Sample).filter(Sample.sha256 == analysis.sample).first()\n",
    "    if sample:\n",
    "        print(f\"\\nSample for Analysis ID {analysis_id}:\")\n",
    "        print(f\"Sample ID: {sample.sha256}, Name: {sample.filepath}\")\n",
    "    else:\n",
    "        print(f\"\\nNo sample found for Analysis ID {analysis_id}.\")\n",
    "    # get all tags for the sample\n",
    "    tags = session.query(Tag).join(sample_tag).join(Sample).filter(Sample.sha256 == sample.sha256).all()\n",
    "    print(f\"\\nTags for Sample ID {sample.sha256}:\")\n",
    "    for tag in tags:\n",
    "        print(f\"Tag: {tag.key}={tag.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0cd1380b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Probabilities:\n",
      "Class 0: 0.0042\n",
      "Class 1: 0.9948\n",
      "Class 2: 0.0000\n",
      "Class 3: 0.0010\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from classifier.models.cnn_nlp import CNN_NLP\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "from transformers import AutoTokenizer\n",
    "classifier_path = '/mnt/data/gimc/classifier/model_data/cnn4bsi_checkpoint.pth'\n",
    "tokenizer_path = '/mnt/data/gimc/classifier/model_data/mal-reformer'\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "TOKENIZER.pad_token = \"[PAD]\"\n",
    "TOKENIZER.cls_token = \"[CLS]\"\n",
    "TOKENIZER.sep_token = \"[SEP]\"\n",
    "\n",
    "checkpoint = torch.load(classifier_path)\n",
    "vocab_size = 20000\n",
    "embed_dim = 128\n",
    "num_classes = 4\n",
    "dropout = 0.5\n",
    "MODEL = CNN_NLP(\n",
    "    pretrained_embedding=None,\n",
    "    freeze_embedding=False,\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    num_filters=[10, 10, 10],\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout\n",
    ")\n",
    "MODEL.load_state_dict(checkpoint['model_states'][-1])\n",
    "MODEL.to(DEVICE)\n",
    "MODEL.eval()\n",
    "\n",
    "def mal_tokenizer(line):\n",
    "    \"\"\"\n",
    "    Tokenize a line of text\n",
    "    \"\"\"\n",
    "    line = line.lower()\n",
    "    line = line.replace(',', ' ')\n",
    "    line = line.replace('\\\\', ' ')\n",
    "    line = line.replace('\\\\\\\\', ' ')\n",
    "    return line.split()\n",
    "\n",
    "dynamic_report_tokenized = []\n",
    "\n",
    "with open(analysis.report) as f:\n",
    "    report = f.read()\n",
    "    dynamic_report = json.loads(report)['dynamic']\n",
    "    for item in dynamic_report:\n",
    "        line = f\"{item['Operation']}, {item['Path']}, {item['Result']}\"\n",
    "        dynamic_report_tokenized.extend(mal_tokenizer(line))\n",
    "\n",
    "report_text = \" \".join(dynamic_report_tokenized)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20480 * 2\n",
    "inputs = TOKENIZER(\n",
    "    report_text,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    return_tensors='pt'\n",
    ").to(DEVICE)\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    logits = MODEL(input_ids)\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "# turn props into a list of probabilities\n",
    "probabilities = probs.cpu().numpy().flatten().tolist()\n",
    "print(\"Class Probabilities:\")\n",
    "for i, prob in enumerate(probabilities):\n",
    "    print(f\"Class {i}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
